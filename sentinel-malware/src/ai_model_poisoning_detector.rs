//! Module 161: AIModelPoisoningDetector â€” On-Device ML Model Integrity & Poisoning Detection
//!
//! Detects tampering, poisoning, and backdoor injection in machine learning
//! models deployed on macOS endpoints. Monitors Core ML models, ONNX models,
//! TensorFlow/PyTorch models, and Apple Intelligence models for integrity
//! violations and adversarial manipulation.
//!
//! ## Detection Capabilities
//!
//! ### Model Integrity
//! - **Hash verification**: Cryptographic hash comparison of model files
//!   against known-good baselines
//! - **Size anomaly**: Model file size deviations indicating modification
//! - **Metadata tampering**: Model metadata field manipulation
//! - **Code signing verification**: Code signature validation for Core ML
//!   models and model bundles
//! - **Provenance verification**: Model origin and supply chain validation
//!
//! ### Backdoor Detection
//! - **Trigger pattern detection**: Statistical analysis of model weights
//!   for embedded trigger patterns (trojan neurons)
//! - **Activation clustering**: Detecting outlier activation patterns that
//!   indicate hidden backdoor behavior
//! - **Neural cleanse analysis**: Detecting minimal perturbation triggers
//!   via optimization-based reverse engineering
//! - **Spectral signature detection**: Singular value analysis of layer
//!   representations for backdoor signatures
//!
//! ### Data Poisoning Indicators
//! - **Training data contamination**: Detecting signs that training data
//!   was poisoned (label flipping, data injection)
//! - **Model drift detection**: Unexpected changes in model behavior
//!   indicating retraining with poisoned data
//! - **Confidence calibration anomaly**: Model confidence scores that
//!   are miscalibrated (overconfident on poisoned inputs)
//!
//! ### Adversarial Robustness
//! - **Adversarial example detection**: Detecting inputs crafted to cause
//!   misclassification (FGSM, PGD, C&W attacks)
//! - **Model extraction attempts**: Detecting systematic querying patterns
//!   aimed at stealing model weights
//! - **Model inversion detection**: Detecting attempts to reconstruct
//!   training data from model outputs
//!
//! ## MITRE ATT&CK: T1195.003, T1059, T1027
//! All 13 sentinel-core breakthroughs integrated.

use crate::types::*;
use sentinel_core::tiered_cache::TieredCache;
use sentinel_core::hierarchical::HierarchicalState;
use sentinel_core::reversible::ReversibleComputation;
use sentinel_core::streaming::StreamAccumulator;
use sentinel_core::differential::DifferentialStore;
use sentinel_core::sparse::SparseMatrix;
use sentinel_core::pruning::PruningMap;
use sentinel_core::dedup::DedupStore;
use sentinel_core::vq_codec::VqCodec;
use sentinel_core::paged::PagedMemory;
use sentinel_core::mmap_stream::StreamingFileProcessor;
use sentinel_core::compression;
use sentinel_core::MemoryMetrics;

use std::collections::{HashMap, HashSet, VecDeque};
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use parking_lot::RwLock;
use tracing::{info, warn, debug};

const HISTORY_LEVELS: u32 = 8;
const HISTORY_PER_LEVEL: usize = 64;
const CACHE_MAX: usize = 50_000;
const STATS_WINDOW: usize = 512;
const VQ_CODEBOOK_SIZE: usize = 128;
const VQ_VECTOR_DIM: usize = 16;
const PAGE_SIZE: usize = 4096;
const MAX_RESIDENT_PAGES: usize = 256;
const MMAP_CHUNK_SIZE: usize = 128 * 1024;

const MODEL_INTEGRITY_INDICATORS: &[(&str, &str, &str, f64)] = &[
    ("hash_mismatch", "Model file hash differs from known-good baseline", "T1195.003", 0.9),
    ("size_anomaly", "Model file size deviates significantly from expected", "T1195.003", 0.7),
    ("metadata_tampered", "Model metadata fields modified unexpectedly", "T1195.003", 0.75),
    ("code_sign_invalid", "Model bundle code signature invalid", "T1195.003", 0.85),
    ("provenance_unknown", "Model origin cannot be verified", "T1195.003", 0.6),
    ("unsigned_coreml", "Unsigned Core ML model loaded", "T1195.003", 0.5),
    ("modified_after_deploy", "Model file modified after deployment", "T1195.003", 0.8),
    ("unexpected_model_format", "Model in unexpected format (possible trojan)", "T1027", 0.7),
    ("model_downgrade", "Model replaced with older/weaker version", "T1195.003", 0.7),
    ("weight_distribution_anomaly", "Model weight statistical distribution anomaly", "T1195.003", 0.65),
];

const BACKDOOR_INDICATORS: &[(&str, &str, &str, f64)] = &[
    ("trojan_neuron_detected", "Trojan neuron pattern in model weights", "T1195.003", 0.9),
    ("activation_cluster_outlier", "Outlier activation cluster (backdoor behavior)", "T1195.003", 0.85),
    ("neural_cleanse_trigger", "Neural Cleanse detected minimal trigger pattern", "T1195.003", 0.9),
    ("spectral_signature_anomaly", "Spectral signature indicates backdoor", "T1195.003", 0.85),
    ("targeted_misclassification", "Model consistently misclassifies specific inputs", "T1195.003", 0.8),
    ("hidden_layer_anomaly", "Hidden layer produces anomalous activations", "T1195.003", 0.7),
    ("weight_clustering_anomaly", "Weight clustering indicates injected pattern", "T1195.003", 0.75),
    ("gradient_anomaly", "Gradient analysis shows backdoor training artifacts", "T1195.003", 0.8),
];

const POISONING_INDICATORS: &[(&str, &str, &str, f64)] = &[
    ("training_data_contamination", "Signs of training data poisoning detected", "T1195.003", 0.8),
    ("label_flip_detected", "Label flipping pattern in model behavior", "T1195.003", 0.75),
    ("model_drift_sudden", "Sudden model behavior drift (possible retraining)", "T1195.003", 0.7),
    ("confidence_miscalibration", "Model confidence scores miscalibrated", "T1195.003", 0.6),
    ("adversarial_example_detected", "Adversarial input detected (crafted misclassification)", "T1059", 0.7),
    ("model_extraction_query_pattern", "Systematic query pattern (model theft attempt)", "T1059", 0.75),
    ("model_inversion_attempt", "Model inversion attack detected", "T1059", 0.8),
    ("data_exfil_via_model", "Data exfiltration via model output encoding", "T1041", 0.8),
];

const MACOS_MODEL_PATHS: &[&str] = &[
    "/Applications/*/Contents/Resources/*.mlmodelc",
    "~/Library/Application Support/*/Models/",
    "/Library/CoreML/Models/",
    "/System/Library/PrivateFrameworks/*/Models/",
    "~/.cache/huggingface/",
    "~/Library/Caches/com.apple.AppleIntelligence/",
];

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, serde::Serialize, serde::Deserialize)]
pub enum ModelThreatType {
    HashMismatch, SizeAnomaly, MetadataTampered, CodeSignInvalid,
    ProvenanceUnknown, ModifiedAfterDeploy, WeightDistributionAnomaly,
    TrojanNeuron, ActivationClusterOutlier, NeuralCleanseTrigger,
    SpectralSignatureAnomaly, TargetedMisclassification,
    TrainingDataContamination, LabelFlip, ModelDriftSudden,
    ConfidenceMiscalibration, AdversarialExample,
    ModelExtractionAttempt, ModelInversionAttempt, DataExfilViaModel,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ModelFinding {
    pub threat_type: ModelThreatType,
    pub confidence: f64,
    pub description: String,
    pub model_path: Option<String>,
    pub model_format: Option<String>,
    pub expected_hash: Option<String>,
    pub actual_hash: Option<String>,
    pub mitre_id: String,
    pub timestamp: u64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ModelEvent {
    pub event_type: String,
    pub model_path: Option<String>,
    pub model_format: Option<String>,
    pub model_size: Option<u64>,
    pub expected_hash: Option<String>,
    pub actual_hash: Option<String>,
    pub code_signed: Option<bool>,
    pub code_sign_valid: Option<bool>,
    pub integrity_check: Option<String>,
    pub process_name: Option<String>,
    pub process_pid: Option<u32>,
    pub file_path: Option<String>,
    pub timestamp: u64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ModelAnalysisResult {
    pub has_threats: bool,
    pub findings: Vec<ModelFinding>,
    pub risk_score: f64,
    pub severity: Severity,
    pub mitre_ids: Vec<String>,
    pub analysis_time_ms: u64,
}

#[derive(Debug, Clone, Default, serde::Serialize, serde::Deserialize)]
pub struct ModelScanStats {
    pub total_events: u64,
    pub integrity_violations: u64,
    pub backdoor_detections: u64,
    pub poisoning_detections: u64,
    pub models_monitored: u64,
    pub avg_analysis_time_ms: f64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ModelSigEntry { pub pattern: String, pub severity: f64 }

pub struct AIModelPoisoningDetector {
    running: Arc<AtomicBool>,
    scan_history: RwLock<HierarchicalState<ModelScanStats>>,
    result_cache: TieredCache<String, ModelAnalysisResult>,
    risk_computer: RwLock<ReversibleComputation<f64, f64>>,
    model_codec: RwLock<VqCodec>,
    rate_accumulator: RwLock<StreamAccumulator<f64, ModelScanStats>>,
    metrics: MemoryMetrics,
    event_diffs: RwLock<DifferentialStore<String, String>>,
    recent_analyses: RwLock<PruningMap<String, ModelAnalysisResult>>,
    sig_db: PagedMemory<ModelSigEntry>,
    file_streamer: StreamingFileProcessor,
    content_dedup: RwLock<DedupStore<String, Vec<u8>>>,
    threat_matrix: RwLock<SparseMatrix<String, String, u64>>,
    stats: RwLock<ModelScanStats>,
    alerts: RwLock<VecDeque<MalwareAlert>>,
    total_events: AtomicU64,
    model_baselines: RwLock<HashMap<String, (String, u64)>>, // path -> (hash, size)
    compressed_artifacts: RwLock<HashMap<String, Vec<u8>>>,
}

impl AIModelPoisoningDetector {
    pub fn new() -> Self {
        Self {
            running: Arc::new(AtomicBool::new(true)),
            scan_history: RwLock::new(HierarchicalState::new(HISTORY_LEVELS, HISTORY_PER_LEVEL)),
            result_cache: TieredCache::new(CACHE_MAX),
            risk_computer: RwLock::new(ReversibleComputation::new(STATS_WINDOW, |v: &[f64]| {
                if v.is_empty() { 0.0 } else { v.iter().sum::<f64>() / v.len() as f64 }
            })),
            model_codec: RwLock::new(VqCodec::new(VQ_CODEBOOK_SIZE, VQ_VECTOR_DIM)),
            rate_accumulator: RwLock::new(StreamAccumulator::new(
                STATS_WINDOW, ModelScanStats::default(),
                |acc: &mut ModelScanStats, vals: &[f64]| { acc.total_events += vals.len() as u64; },
            )),
            metrics: MemoryMetrics::new(64 * 1024 * 1024),
            event_diffs: RwLock::new(DifferentialStore::new()),
            recent_analyses: RwLock::new(PruningMap::new(CACHE_MAX)),
            sig_db: PagedMemory::new(PAGE_SIZE, MAX_RESIDENT_PAGES),
            file_streamer: StreamingFileProcessor::new(MMAP_CHUNK_SIZE),
            content_dedup: RwLock::new(DedupStore::new()),
            threat_matrix: RwLock::new(SparseMatrix::new(0u64)),
            stats: RwLock::new(ModelScanStats::default()),
            alerts: RwLock::new(VecDeque::with_capacity(128)),
            total_events: AtomicU64::new(0),
            model_baselines: RwLock::new(HashMap::new()),
            compressed_artifacts: RwLock::new(HashMap::new()),
        }
    }

    pub fn analyze_event(&self, event: &ModelEvent) -> Option<ModelAnalysisResult> {
        if !self.running.load(Ordering::SeqCst) { return None; }
        let start = std::time::Instant::now();
        self.total_events.fetch_add(1, Ordering::Relaxed);
        self.scan_history.write().checkpoint(self.stats.read().clone());

        let cache_key = format!("model:{}:{}", event.event_type, event.timestamp);
        let mut findings = Vec::new();
        let mut mitre_ids = HashSet::new();

        let ind_text = format!("{} {}",
            event.event_type, event.integrity_check.as_deref().unwrap_or(""));
        let ind_lower = ind_text.to_lowercase();

        // 1. Integrity indicators
        for &(pattern, desc, mitre, conf) in MODEL_INTEGRITY_INDICATORS {
            if ind_lower.contains(&pattern.to_lowercase()) {
                let tt = match pattern {
                    "hash_mismatch" => ModelThreatType::HashMismatch,
                    "size_anomaly" => ModelThreatType::SizeAnomaly,
                    "metadata_tampered" => ModelThreatType::MetadataTampered,
                    "code_sign_invalid" | "unsigned_coreml" => ModelThreatType::CodeSignInvalid,
                    "provenance_unknown" => ModelThreatType::ProvenanceUnknown,
                    "modified_after_deploy" => ModelThreatType::ModifiedAfterDeploy,
                    "weight_distribution_anomaly" => ModelThreatType::WeightDistributionAnomaly,
                    _ => ModelThreatType::HashMismatch,
                };
                findings.push(ModelFinding {
                    threat_type: tt, confidence: conf, description: desc.to_string(),
                    model_path: event.model_path.clone(), model_format: event.model_format.clone(),
                    expected_hash: event.expected_hash.clone(), actual_hash: event.actual_hash.clone(),
                    mitre_id: mitre.into(), timestamp: event.timestamp,
                });
                mitre_ids.insert(mitre.into());
            }
        }

        // Direct hash check
        if let (Some(ref exp), Some(ref act)) = (&event.expected_hash, &event.actual_hash) {
            if exp != act {
                findings.push(ModelFinding {
                    threat_type: ModelThreatType::HashMismatch, confidence: 0.9,
                    description: format!("Model hash mismatch: expected {} got {}", &exp[..8.min(exp.len())], &act[..8.min(act.len())]),
                    model_path: event.model_path.clone(), model_format: event.model_format.clone(),
                    expected_hash: Some(exp.clone()), actual_hash: Some(act.clone()),
                    mitre_id: "T1195.003".into(), timestamp: event.timestamp,
                });
                mitre_ids.insert("T1195.003".into());
            }
        }

        // Code sign check
        if let (Some(true), Some(false)) = (event.code_signed, event.code_sign_valid) {
            findings.push(ModelFinding {
                threat_type: ModelThreatType::CodeSignInvalid, confidence: 0.85,
                description: "Model code signature is invalid".into(),
                model_path: event.model_path.clone(), model_format: event.model_format.clone(),
                expected_hash: None, actual_hash: None,
                mitre_id: "T1195.003".into(), timestamp: event.timestamp,
            });
            mitre_ids.insert("T1195.003".into());
        }

        // 2. Backdoor indicators
        for &(pattern, desc, mitre, conf) in BACKDOOR_INDICATORS {
            if ind_lower.contains(&pattern.to_lowercase()) {
                let tt = match pattern {
                    "trojan_neuron_detected" => ModelThreatType::TrojanNeuron,
                    "activation_cluster_outlier" => ModelThreatType::ActivationClusterOutlier,
                    "neural_cleanse_trigger" => ModelThreatType::NeuralCleanseTrigger,
                    "spectral_signature_anomaly" => ModelThreatType::SpectralSignatureAnomaly,
                    "targeted_misclassification" => ModelThreatType::TargetedMisclassification,
                    _ => ModelThreatType::TrojanNeuron,
                };
                findings.push(ModelFinding {
                    threat_type: tt, confidence: conf, description: desc.to_string(),
                    model_path: event.model_path.clone(), model_format: event.model_format.clone(),
                    expected_hash: None, actual_hash: None,
                    mitre_id: mitre.into(), timestamp: event.timestamp,
                });
                mitre_ids.insert(mitre.into());
            }
        }

        // 3. Poisoning indicators
        for &(pattern, desc, mitre, conf) in POISONING_INDICATORS {
            if ind_lower.contains(&pattern.to_lowercase()) {
                let tt = match pattern {
                    "training_data_contamination" => ModelThreatType::TrainingDataContamination,
                    "label_flip_detected" => ModelThreatType::LabelFlip,
                    "model_drift_sudden" => ModelThreatType::ModelDriftSudden,
                    "confidence_miscalibration" => ModelThreatType::ConfidenceMiscalibration,
                    "adversarial_example_detected" => ModelThreatType::AdversarialExample,
                    "model_extraction_query_pattern" => ModelThreatType::ModelExtractionAttempt,
                    "model_inversion_attempt" => ModelThreatType::ModelInversionAttempt,
                    "data_exfil_via_model" => ModelThreatType::DataExfilViaModel,
                    _ => ModelThreatType::TrainingDataContamination,
                };
                findings.push(ModelFinding {
                    threat_type: tt, confidence: conf, description: desc.to_string(),
                    model_path: event.model_path.clone(), model_format: event.model_format.clone(),
                    expected_hash: None, actual_hash: None,
                    mitre_id: mitre.into(), timestamp: event.timestamp,
                });
                mitre_ids.insert(mitre.into());
            }
        }

        let risk_score = if findings.is_empty() { 0.0 } else {
            let s: f64 = findings.iter().map(|f| f.confidence * match f.threat_type {
                ModelThreatType::TrojanNeuron | ModelThreatType::NeuralCleanseTrigger => 1.5,
                ModelThreatType::HashMismatch | ModelThreatType::CodeSignInvalid => 1.4,
                ModelThreatType::DataExfilViaModel => 1.4,
                _ => 1.0,
            }).sum();
            (s / (findings.len() as f64 * 1.5)).min(1.0)
        };
        self.risk_computer.write().push(risk_score);
        let severity = if risk_score >= 0.85 { Severity::Critical } else if risk_score >= 0.65 { Severity::High }
            else if risk_score >= 0.45 { Severity::Medium } else if risk_score >= 0.25 { Severity::Low }
            else { Severity::Info };
        let has_threats = risk_score > 0.5;
        let mitre_vec: Vec<String> = mitre_ids.into_iter().collect();
        let elapsed = start.elapsed().as_millis() as u64;

        let result = ModelAnalysisResult {
            has_threats, findings, risk_score,
            severity: severity.clone(), mitre_ids: mitre_vec.clone(), analysis_time_ms: elapsed,
        };

        self.result_cache.insert(cache_key.clone(), result.clone());
        self.recent_analyses.write().insert_with_priority(cache_key.clone(), result.clone(), risk_score);
        self.rate_accumulator.write().push(risk_score);

        { let mut s = self.stats.write(); s.total_events += 1;
          s.models_monitored = self.model_baselines.read().len() as u64;
          if has_threats { for f in &result.findings { match f.threat_type {
              ModelThreatType::HashMismatch | ModelThreatType::SizeAnomaly |
              ModelThreatType::MetadataTampered | ModelThreatType::CodeSignInvalid |
              ModelThreatType::ModifiedAfterDeploy => s.integrity_violations += 1,
              ModelThreatType::TrojanNeuron | ModelThreatType::ActivationClusterOutlier |
              ModelThreatType::NeuralCleanseTrigger | ModelThreatType::SpectralSignatureAnomaly => s.backdoor_detections += 1,
              _ => s.poisoning_detections += 1,
          } } }
          let n = s.total_events as f64;
          s.avg_analysis_time_ms = s.avg_analysis_time_ms * ((n-1.0)/n) + elapsed as f64 / n;
        }

        if has_threats {
            self.alerts.write().push_back(MalwareAlert {
                id: uuid::Uuid::new_v4().to_string(), timestamp: chrono::Utc::now().timestamp(), severity,
                module: "ai_model_poisoning_detector".into(),
                title: format!("AI MODEL THREAT: {} findings in {:?}", result.findings.len(), event.model_path),
                details: format!("Risk {:.1}%, format: {:?}", risk_score*100.0, event.model_format),
                path: event.file_path.clone(), process_name: event.process_name.clone(),
                process_pid: event.process_pid, verdict: None, mitre_ids: mitre_vec,
                remediation: vec![
                    "Quarantine the modified model file".into(),
                    "Restore model from verified backup/origin".into(),
                    "Re-verify model hash against trusted source".into(),
                    "Run Neural Cleanse or Spectral Signatures analysis".into(),
                ], confidence: risk_score,
            });
        }
        Some(result)
    }

    pub fn stats(&self) -> ModelScanStats { self.stats.read().clone() }
    pub fn drain_alerts(&self) -> Vec<MalwareAlert> { self.alerts.write().drain(..).collect() }
    pub fn stop(&self) { self.running.store(false, Ordering::SeqCst); }
}
