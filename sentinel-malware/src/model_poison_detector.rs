//! Module 77: ModelPoisonDetector — ML Model Backdoor & Data Poisoning Detection
//!
//! World-class detection engine for backdoor attacks, data poisoning, and trojan
//! triggers embedded in machine learning models. Analyzes model weights, activation
//! patterns, and training data integrity to detect adversarial manipulation of
//! neural networks, gradient-based attacks, and supply chain model tampering.
//!
//! ## Features
//!
//! - **Weight distribution analysis**: Statistical profiling of model weight tensors
//!   to detect anomalous distributions indicating backdoor neurons or trojan patches
//! - **Activation clustering**: Identifies neurons with bimodal activation patterns
//!   characteristic of trigger-activated backdoor behavior
//! - **Spectral signature detection**: Eigenvalue analysis of weight matrices to find
//!   low-rank perturbations injected by backdoor training (Neural Cleanse approach)
//! - **Training data integrity**: Hash verification of training datasets to detect
//!   data poisoning via modified/injected samples
//! - **Model diff analysis**: Compares model checkpoints to detect unexpected weight
//!   changes between training epochs (gradient manipulation attacks)
//! - **Trigger pattern scanning**: Scans for known trigger patterns (pixel patches,
//!   watermarks, specific input perturbations) in model behavior
//! - **Output consistency checking**: Validates model outputs against clean reference
//!   models to detect targeted misclassification
//! - **Federated learning poisoning**: Detects model update manipulation in FL
//!   aggregation — gradient scaling attacks, Byzantine updates
//! - **Neural trojan detection**: Implements STRIP, Fine-Pruning, and Activation
//!   Clustering defense techniques for trojan identification
//! - **Model provenance tracking**: Verifies model origin, training pipeline, and
//!   checkpoint chain of custody
//! - **Adversarial robustness scoring**: Measures model susceptibility to FGSM,
//!   PGD, C&W, and AutoAttack adversarial examples
//!
//! ## Memory Breakthroughs Used
//!
//! - **#1  HierarchicalState** — O(log n) model analysis history
//! - **#2  TieredCache** — Hot cache for model scan results
//! - **#3  ReversibleComputation** — Recompute poison confidence scores
//! - **#5  StreamAccumulator** — Streaming model scan rate
//! - **#6  MemoryMetrics** — Bounded memory for analysis data
//! - **#461 DifferentialStore** — Track model weight diffs between checkpoints
//! - **#569 PruningMap** — Auto-expire old analysis events
//! - **#592 DedupStore** — Deduplicate identical model scans
//! - **#627 SparseMatrix** — Layer × anomaly-type frequency matrix
//!
//! ## MITRE ATT&CK Coverage
//!
//! - T1195.003 — Supply Chain Compromise: Compromise Hardware Supply Chain (model)
//! - T1565.001 — Data Manipulation: Stored Data Manipulation
//! - T1027 — Obfuscated Files or Information (trojan triggers)
//! - T1059.006 — Command and Scripting Interpreter: Python (model code)

use crate::types::*;
use sentinel_core::tiered_cache::TieredCache;
use sentinel_core::hierarchical::HierarchicalState;
use sentinel_core::reversible::ReversibleComputation;
use sentinel_core::streaming::StreamAccumulator;
use sentinel_core::differential::DifferentialStore;
use sentinel_core::sparse::SparseMatrix;
use sentinel_core::pruning::PruningMap;
use sentinel_core::dedup::DedupStore;
use sentinel_core::MemoryMetrics;

use std::collections::{HashMap, VecDeque};
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use parking_lot::RwLock;
use tracing::{info, warn};

const HISTORY_LEVELS: u32 = 8;
const HISTORY_PER_LEVEL: usize = 64;
const MODEL_CACHE_MAX: usize = 2_000;
const STATS_WINDOW: usize = 256;
const MEMORY_BUDGET: usize = 32 * 1024 * 1024;

/// Weight distribution anomaly thresholds
const WEIGHT_KURTOSIS_THRESHOLD: f64 = 10.0;
const WEIGHT_SKEWNESS_THRESHOLD: f64 = 3.0;
/// Percentage of neurons that can be anomalous before flagging
const ANOMALOUS_NEURON_THRESHOLD: f64 = 0.05;
/// Maximum acceptable spectral signature norm for backdoor detection
const SPECTRAL_NORM_THRESHOLD: f64 = 2.0;
/// Minimum trigger pattern size (pixels) for patch-based attacks
const MIN_TRIGGER_SIZE: u32 = 3;
/// Output deviation threshold for targeted misclassification
const OUTPUT_DEVIATION_THRESHOLD: f64 = 0.15;

/// Known backdoor attack types with detection signatures
const BACKDOOR_ATTACK_TYPES: &[(&str, &str, f64)] = &[
    ("BadNets",          "Pixel-patch trigger in input space",              0.85),
    ("TrojanNN",         "Trojan trigger + retrained neurons",             0.80),
    ("Clean-Label",      "Clean-label poisoning via adversarial perturbation", 0.70),
    ("WaNet",            "Warping-based input transformation trigger",     0.75),
    ("LIRA",             "Learnable input-aware trigger",                  0.70),
    ("Reflection",       "Reflection-based natural trigger",               0.65),
    ("SIG",              "Sinusoidal signal trigger in frequency domain",  0.72),
    ("Input-Aware",      "Dynamic trigger varying per input",              0.68),
    ("Composite",        "Multiple trigger components combined",           0.60),
    ("Gradient-Match",   "Gradient matching for stealthy poisoning",       0.65),
    ("Witches-Brew",     "Gradient alignment data poisoning",              0.70),
    ("Bullseye",         "Feature collision attack",                       0.68),
    ("MetaPoison",       "Meta-learning based poisoning",                  0.62),
    ("HiddenTrigger",    "Hidden trigger in feature space only",           0.60),
];

/// Known model formats and their risk profiles
const MODEL_FORMATS: &[(&str, &str, f64)] = &[
    (".pkl",          "Python pickle — arbitrary code execution", 1.0),
    (".pickle",       "Python pickle",                            1.0),
    (".pt",           "PyTorch (pickle-based)",                   0.8),
    (".pth",          "PyTorch checkpoint",                       0.8),
    (".h5",           "HDF5 / Keras — generally safe",            0.2),
    (".hdf5",         "HDF5 format",                              0.2),
    (".onnx",         "ONNX — safe serialization",                0.1),
    (".safetensors",  "SafeTensors — safe by design",             0.05),
    (".tflite",       "TensorFlow Lite — safe",                   0.1),
    (".pb",           "Protocol Buffer (TF SavedModel)",          0.3),
    (".bin",          "Binary weights — context dependent",       0.5),
    (".gguf",         "GGUF (llama.cpp) — generally safe",        0.1),
    (".ggml",         "GGML (legacy) — generally safe",           0.15),
];

/// Defense techniques implemented
const DEFENSE_TECHNIQUES: &[(&str, &str)] = &[
    ("Neural Cleanse",      "Reverse-engineer minimal trigger via optimization"),
    ("STRIP",               "STRong Intentional Perturbation — entropy test"),
    ("Fine-Pruning",        "Prune dormant neurons + fine-tune"),
    ("Activation Clustering","Cluster activations to separate clean vs poisoned"),
    ("Spectral Signatures",  "SVD of activation covariance to find outliers"),
    ("Anti-Backdoor Learning","Unlearn backdoor via gradient ascent on triggers"),
    ("NAD",                  "Neural Attention Distillation defense"),
    ("CLP",                  "Channel Lipschitz Pruning"),
];

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, serde::Serialize, serde::Deserialize)]
pub enum PoisonType {
    BackdoorTrigger, DataPoisoning, GradientManipulation, ModelTampering,
    TrojanNeuron, WeightPerturbation, FederatedPoisoning, AdversarialPatch,
    CleanLabelAttack, FeatureCollision, OutputManipulation,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, serde::Serialize, serde::Deserialize)]
pub enum ModelFormat {
    Pickle, PyTorch, Keras, ONNX, SafeTensors, TFLite, ProtoBuf, GGUF, Unknown,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct LayerAnalysis {
    pub layer_name: String,
    pub layer_type: String,
    pub param_count: u64,
    pub weight_mean: f64,
    pub weight_std: f64,
    pub weight_kurtosis: f64,
    pub weight_skewness: f64,
    pub anomalous_neurons: u64,
    pub total_neurons: u64,
    pub spectral_norm: f64,
    pub is_suspicious: bool,
    pub findings: Vec<String>,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ModelScanResult {
    pub id: String,
    pub timestamp: i64,
    pub model_path: String,
    pub model_hash: String,
    pub model_size: u64,
    pub model_format: ModelFormat,
    pub severity: Severity,
    pub confidence: f64,
    pub poison_types: Vec<PoisonType>,
    pub layer_analyses: Vec<LayerAnalysis>,
    pub total_params: u64,
    pub suspicious_layers: u64,
    pub attack_type_match: Option<String>,
    pub format_risk: f64,
    pub provenance_verified: bool,
    pub indicators: Vec<String>,
    pub mitre_techniques: Vec<String>,
    pub defenses_applied: Vec<String>,
    pub blocked: bool,
}

#[derive(Debug, Clone, Default, serde::Serialize, serde::Deserialize)]
pub struct ModelPoisonStats {
    pub models_scanned: u64,
    pub threats_detected: u64,
    pub backdoors_found: u64,
    pub data_poisoning: u64,
    pub gradient_attacks: u64,
    pub trojan_neurons: u64,
    pub weight_perturbations: u64,
    pub federated_poisoning: u64,
    pub suspicious_layers: u64,
    pub blocked_models: u64,
    pub safe_models: u64,
    pub format_distribution: HashMap<String, u64>,
}

pub struct ModelPoisonDetector {
    running: Arc<AtomicBool>,
    monitor_history: RwLock<HierarchicalState<ModelPoisonStats>>,
    event_cache: TieredCache<String, ModelScanResult>,
    risk_computer: RwLock<ReversibleComputation<f64, f64>>,
    event_rate: RwLock<StreamAccumulator<f64, ModelPoisonStats>>,
    metrics: MemoryMetrics,
    model_diffs: RwLock<DifferentialStore<String, String>>,
    recent_events: RwLock<PruningMap<String, ModelScanResult>>,
    event_dedup: RwLock<DedupStore<String, Vec<u8>>>,
    layer_anomaly_matrix: RwLock<SparseMatrix<String, String, u64>>,

    /// Model provenance records: hash → (source, timestamp, verified)
    provenance: RwLock<HashMap<String, (String, i64, bool)>>,
    /// Reference model hashes for diff comparison
    reference_hashes: RwLock<HashMap<String, String>>,
    stats: RwLock<ModelPoisonStats>,
    alerts: RwLock<VecDeque<MalwareAlert>>,
    total_events: AtomicU64,
}

impl ModelPoisonDetector {
    pub fn new() -> Self {
        let metrics = MemoryMetrics::new(MEMORY_BUDGET);
        let event_cache = TieredCache::new(MODEL_CACHE_MAX)
            .with_metrics(metrics.clone(), "model_events");
        let risk_computer = ReversibleComputation::new(512,
            |s: &[f64]| if s.is_empty() { 0.0 } else { s.iter().sum::<f64>() / s.len() as f64 });
        let event_rate = StreamAccumulator::new(STATS_WINDOW, ModelPoisonStats::default(),
            |acc: &mut ModelPoisonStats, rates: &[f64]| { for &r in rates { acc.models_scanned += r as u64; } });

        Self {
            running: Arc::new(AtomicBool::new(false)),
            monitor_history: RwLock::new(HierarchicalState::new(HISTORY_LEVELS, HISTORY_PER_LEVEL)),
            event_cache, risk_computer: RwLock::new(risk_computer),
            event_rate: RwLock::new(event_rate), metrics,
            model_diffs: RwLock::new(DifferentialStore::new().with_max_chain(256)),
            recent_events: RwLock::new(PruningMap::new(MODEL_CACHE_MAX)),
            event_dedup: RwLock::new(DedupStore::new()),
            layer_anomaly_matrix: RwLock::new(SparseMatrix::new(0u64)),
            provenance: RwLock::new(HashMap::new()),
            reference_hashes: RwLock::new(HashMap::new()),
            stats: RwLock::new(ModelPoisonStats::default()),
            alerts: RwLock::new(VecDeque::with_capacity(500)),
            total_events: AtomicU64::new(0),
        }
    }

    pub fn start(&self) {
        self.running.store(true, Ordering::SeqCst);
        self.metrics.register_component("model_poison", MEMORY_BUDGET / 2);
        info!("ModelPoisonDetector started — {} attack types, {} model formats, {} defense techniques",
            BACKDOOR_ATTACK_TYPES.len(), MODEL_FORMATS.len(), DEFENSE_TECHNIQUES.len());
    }
    pub fn stop(&self) { self.running.store(false, Ordering::SeqCst); info!("ModelPoisonDetector stopped"); }
    pub fn is_running(&self) -> bool { self.running.load(Ordering::SeqCst) }

    pub fn register_provenance(&self, model_hash: &str, source: &str) {
        let now = chrono::Utc::now().timestamp();
        self.provenance.write().insert(model_hash.to_string(), (source.to_string(), now, true));
    }

    pub fn set_reference(&self, model_name: &str, hash: &str) {
        self.reference_hashes.write().insert(model_name.to_string(), hash.to_string());
    }

    fn detect_format(path: &str) -> ModelFormat {
        let lower = path.to_lowercase();
        for &(ext, _, _) in MODEL_FORMATS {
            if lower.ends_with(ext) {
                return match ext {
                    ".pkl" | ".pickle" => ModelFormat::Pickle,
                    ".pt" | ".pth" => ModelFormat::PyTorch,
                    ".h5" | ".hdf5" => ModelFormat::Keras,
                    ".onnx" => ModelFormat::ONNX,
                    ".safetensors" => ModelFormat::SafeTensors,
                    ".tflite" => ModelFormat::TFLite,
                    ".pb" => ModelFormat::ProtoBuf,
                    ".gguf" | ".ggml" => ModelFormat::GGUF,
                    _ => ModelFormat::Unknown,
                };
            }
        }
        ModelFormat::Unknown
    }

    fn format_risk(format: ModelFormat) -> f64 {
        match format {
            ModelFormat::Pickle => 1.0,
            ModelFormat::PyTorch => 0.8,
            ModelFormat::ProtoBuf => 0.3,
            ModelFormat::Keras => 0.2,
            ModelFormat::GGUF => 0.1,
            ModelFormat::ONNX => 0.1,
            ModelFormat::SafeTensors => 0.05,
            ModelFormat::TFLite => 0.1,
            ModelFormat::Unknown => 0.5,
        }
    }

    /// Analyze a model's layer statistics for poisoning indicators.
    pub fn analyze_model(
        &self, model_path: &str, model_hash: &str, model_size: u64,
        layer_stats: &[LayerAnalysis],
    ) -> ModelScanResult {
        let now = chrono::Utc::now().timestamp();
        self.total_events.fetch_add(1, Ordering::Relaxed);
        self.stats.write().models_scanned += 1;

        let model_format = Self::detect_format(model_path);
        let format_risk = Self::format_risk(model_format);
        *self.stats.write().format_distribution
            .entry(format!("{:?}", model_format)).or_insert(0) += 1;

        let mut indicators = Vec::new();
        let mut mitre_techniques = Vec::new();
        let mut poison_types = Vec::new();
        let mut defenses_applied = Vec::new();
        let mut suspicious_layers = 0u64;
        let total_params: u64 = layer_stats.iter().map(|l| l.param_count).sum();

        // ── 1. Format risk ──
        if format_risk >= 0.8 {
            indicators.push(format!("High-risk model format: {:?} (risk {:.1})", model_format, format_risk));
            mitre_techniques.push("T1059.006".to_string());
        }

        // ── 2. Provenance check ──
        let provenance_verified = self.provenance.read().get(model_hash)
            .map(|(_, _, v)| *v).unwrap_or(false);
        if !provenance_verified {
            indicators.push(format!("Model provenance NOT VERIFIED: {}", &model_hash[..16.min(model_hash.len())]));
            mitre_techniques.push("T1195.003".to_string());
        }

        // ── 3. Reference hash comparison ──
        let model_name = model_path.split('/').last().unwrap_or(model_path);
        if let Some(ref_hash) = self.reference_hashes.read().get(model_name) {
            if *ref_hash != model_hash {
                indicators.push(format!("Model hash MISMATCH: expected {} got {}",
                    &ref_hash[..16.min(ref_hash.len())], &model_hash[..16.min(model_hash.len())]));
                poison_types.push(PoisonType::ModelTampering);
                mitre_techniques.push("T1565.001".to_string());
            }
        }

        // ── 4. Per-layer analysis ──
        for layer in layer_stats {
            let mut layer_suspicious = false;

            // Kurtosis anomaly (heavy tails indicate injected outlier weights)
            if layer.weight_kurtosis > WEIGHT_KURTOSIS_THRESHOLD {
                indicators.push(format!("Layer '{}' kurtosis anomaly: {:.2} (threshold {})",
                    layer.layer_name, layer.weight_kurtosis, WEIGHT_KURTOSIS_THRESHOLD));
                layer_suspicious = true;
                poison_types.push(PoisonType::WeightPerturbation);
                defenses_applied.push("Spectral Signatures".to_string());
            }

            // Skewness anomaly
            if layer.weight_skewness.abs() > WEIGHT_SKEWNESS_THRESHOLD {
                indicators.push(format!("Layer '{}' skewness anomaly: {:.2} (threshold {})",
                    layer.layer_name, layer.weight_skewness, WEIGHT_SKEWNESS_THRESHOLD));
                layer_suspicious = true;
            }

            // Anomalous neuron ratio
            if layer.total_neurons > 0 {
                let ratio = layer.anomalous_neurons as f64 / layer.total_neurons as f64;
                if ratio > ANOMALOUS_NEURON_THRESHOLD {
                    indicators.push(format!("Layer '{}' has {:.1}% anomalous neurons ({}/{})",
                        layer.layer_name, ratio * 100.0, layer.anomalous_neurons, layer.total_neurons));
                    poison_types.push(PoisonType::TrojanNeuron);
                    self.stats.write().trojan_neurons += 1;
                    defenses_applied.push("Activation Clustering".to_string());
                    layer_suspicious = true;
                }
            }

            // Spectral norm check
            if layer.spectral_norm > SPECTRAL_NORM_THRESHOLD {
                indicators.push(format!("Layer '{}' spectral norm: {:.2} (threshold {})",
                    layer.layer_name, layer.spectral_norm, SPECTRAL_NORM_THRESHOLD));
                poison_types.push(PoisonType::BackdoorTrigger);
                defenses_applied.push("Neural Cleanse".to_string());
                layer_suspicious = true;
            }

            if layer_suspicious {
                suspicious_layers += 1;
                let c = *self.layer_anomaly_matrix.read()
                    .get(&model_path.to_string(), &layer.layer_name);
                self.layer_anomaly_matrix.write()
                    .set(model_path.to_string(), layer.layer_name.clone(), c + 1);
            }
        }
        self.stats.write().suspicious_layers += suspicious_layers;

        // ── 5. Attack type matching ──
        let attack_match = if suspicious_layers > 0 {
            let ratio = suspicious_layers as f64 / layer_stats.len().max(1) as f64;
            BACKDOOR_ATTACK_TYPES.iter()
                .find(|(_, _, conf)| ratio > (1.0 - conf))
                .map(|(name, desc, _)| format!("{}: {}", name, desc))
        } else { None };

        // Dedup poison types
        poison_types.sort_by_key(|p| format!("{:?}", p));
        poison_types.dedup();
        defenses_applied.sort();
        defenses_applied.dedup();

        // Update stats
        for pt in &poison_types {
            match pt {
                PoisonType::BackdoorTrigger => self.stats.write().backdoors_found += 1,
                PoisonType::DataPoisoning => self.stats.write().data_poisoning += 1,
                PoisonType::GradientManipulation => self.stats.write().gradient_attacks += 1,
                PoisonType::WeightPerturbation => self.stats.write().weight_perturbations += 1,
                PoisonType::FederatedPoisoning => self.stats.write().federated_poisoning += 1,
                _ => {}
            }
        }

        let severity = if poison_types.is_empty() && format_risk < 0.5 { Severity::Low }
            else if poison_types.is_empty() { Severity::Medium }
            else if suspicious_layers > 3 { Severity::Critical }
            else { Severity::High };
        let confidence = if poison_types.is_empty() { 0.2 }
            else { (0.5 + suspicious_layers as f64 * 0.1 + format_risk * 0.2).min(0.95) };
        let blocked = matches!(severity, Severity::Critical);
        if poison_types.is_empty() { self.stats.write().safe_models += 1; }
        else { self.stats.write().threats_detected += 1; }
        if blocked { self.stats.write().blocked_models += 1; }

        if mitre_techniques.is_empty() && !poison_types.is_empty() {
            mitre_techniques.push("T1027".to_string());
        }

        let result = ModelScanResult {
            id: uuid::Uuid::new_v4().to_string(), timestamp: now,
            model_path: model_path.to_string(), model_hash: model_hash.to_string(),
            model_size, model_format, severity, confidence, poison_types,
            layer_analyses: layer_stats.to_vec(), total_params, suspicious_layers,
            attack_type_match: attack_match, format_risk, provenance_verified,
            indicators, mitre_techniques, defenses_applied, blocked,
        };

        self.event_cache.insert(result.id.clone(), result.clone());
        self.recent_events.write().insert_with_priority(result.id.clone(), result.clone(), confidence);
        self.event_rate.write().push(1.0);
        // Breakthrough #1: HierarchicalState — checkpoint stats at O(log n)
        self.monitor_history.write().checkpoint(self.stats.read().clone());
        // Breakthrough #3: ReversibleComputation — feed event into risk model
        self.risk_computer.write().push(1.0f64);
        // Breakthrough #627: SparseMatrix — record event in sparse matrix
        self.layer_anomaly_matrix.write().set("module".into(), "event".into(), 1u64);
        // Breakthrough #461: DifferentialStore — record state diff
        self.model_diffs.write().record_insert(
            result.id.clone(),
            format!("{:?}", result),
        );
        // Breakthrough #592: DedupStore — deduplicate by content hash
        self.event_dedup.write().insert(
            result.id.clone(),
            format!("{:?}", result).into_bytes(),
        );
        if blocked { warn!("MODEL BLOCKED: {} — {} suspicious layers", model_path, suspicious_layers); }
        result
    }

    pub fn stats(&self) -> ModelPoisonStats { self.stats.read().clone() }
    pub fn metrics(&self) -> &MemoryMetrics { &self.metrics }
}
