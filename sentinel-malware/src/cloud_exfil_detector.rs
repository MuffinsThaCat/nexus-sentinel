//! Module 123: CloudExfilDetector — Cloud Storage & SaaS Exfiltration Detection
//!
//! Detects data exfiltration via cloud storage services (S3, GCS, Azure Blob,
//! Dropbox, OneDrive, Google Drive, Box, etc.) and SaaS platforms. Attackers
//! increasingly use legitimate cloud services as exfiltration channels because
//! the traffic blends with normal business usage.
//!
//! ## Detection Capabilities
//!
//! - **Cloud storage uploads**: Unusual large uploads to S3, GCS, Azure Blob,
//!   Dropbox, OneDrive, Google Drive, Box, Mega, WeTransfer
//! - **Unauthorized cloud accounts**: Data flowing to personal/unknown cloud
//!   accounts vs corporate-sanctioned ones
//! - **Volume anomaly**: Sudden spike in outbound data to cloud services
//! - **After-hours exfiltration**: Cloud uploads during non-business hours
//! - **Sensitive file patterns**: Upload of archives, databases, source code,
//!   credential files, PII-containing documents
//! - **API key abuse**: Unauthorized use of cloud API keys for uploads
//! - **SaaS channel abuse**: Slack, Teams, Discord, Telegram used as exfil
//!   channels with file attachments or webhook data
//! - **Encrypted archive uploads**: Password-protected ZIP/RAR/7z uploaded
//!   to cloud (hiding content from DLP)
//! - **Chunked uploads**: Data split across multiple small uploads to evade
//!   size-based detection
//! - **Staging detection**: Local staging of data before bulk cloud upload
//!
//! ## Memory Breakthroughs Used
//!
//! All 13 sentinel-core breakthroughs are integrated.

use crate::types::*;
use sentinel_core::tiered_cache::TieredCache;
use sentinel_core::hierarchical::HierarchicalState;
use sentinel_core::reversible::ReversibleComputation;
use sentinel_core::streaming::StreamAccumulator;
use sentinel_core::differential::DifferentialStore;
use sentinel_core::sparse::SparseMatrix;
use sentinel_core::pruning::PruningMap;
use sentinel_core::dedup::DedupStore;
use sentinel_core::vq_codec::VqCodec;
use sentinel_core::paged::PagedMemory;
use sentinel_core::mmap_stream::StreamingFileProcessor;
use sentinel_core::compression;
use sentinel_core::MemoryMetrics;

use std::collections::{HashMap, HashSet, VecDeque};
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use parking_lot::RwLock;
use tracing::{info, warn, debug};

const HISTORY_LEVELS: u32 = 7;
const HISTORY_PER_LEVEL: usize = 48;
const CACHE_MAX: usize = 30_000;
const STATS_WINDOW: usize = 256;
const VQ_CODEBOOK_SIZE: usize = 64;
const VQ_VECTOR_DIM: usize = 12;
const PAGE_SIZE: usize = 4096;
const MAX_RESIDENT_PAGES: usize = 128;
const MMAP_CHUNK_SIZE: usize = 64 * 1024;
const LARGE_UPLOAD_THRESHOLD: u64 = 50 * 1024 * 1024; // 50MB
const DAILY_UPLOAD_THRESHOLD: u64 = 500 * 1024 * 1024; // 500MB/day
const RAPID_UPLOAD_COUNT: u32 = 20;
const RAPID_UPLOAD_WINDOW_SECS: u64 = 300;

// ── Cloud Service Domains ────────────────────────────────────────────────────

const CLOUD_STORAGE_DOMAINS: &[(&str, &str, &str, f64)] = &[
    // AWS S3
    ("s3.amazonaws.com", "AWS S3", "storage", 0.4),
    ("s3-*.amazonaws.com", "AWS S3 regional", "storage", 0.4),
    (".s3.amazonaws.com", "AWS S3 bucket", "storage", 0.4),
    // Google Cloud Storage
    ("storage.googleapis.com", "Google Cloud Storage", "storage", 0.4),
    ("storage.cloud.google.com", "Google Cloud Storage", "storage", 0.4),
    // Azure Blob
    (".blob.core.windows.net", "Azure Blob Storage", "storage", 0.4),
    (".file.core.windows.net", "Azure Files", "storage", 0.4),
    // Dropbox
    ("dropbox.com", "Dropbox", "personal_cloud", 0.5),
    ("dropboxapi.com", "Dropbox API", "personal_cloud", 0.6),
    ("content.dropboxapi.com", "Dropbox content API", "personal_cloud", 0.65),
    // Google Drive
    ("drive.google.com", "Google Drive", "personal_cloud", 0.5),
    ("www.googleapis.com/upload", "Google Drive upload API", "personal_cloud", 0.6),
    // OneDrive
    ("onedrive.live.com", "OneDrive Personal", "personal_cloud", 0.55),
    ("graph.microsoft.com", "Microsoft Graph (OneDrive)", "enterprise", 0.3),
    // Box
    ("box.com", "Box", "enterprise", 0.3),
    ("upload.box.com", "Box upload", "enterprise", 0.4),
    // Mega
    ("mega.nz", "MEGA", "personal_cloud", 0.7),
    ("mega.co.nz", "MEGA legacy", "personal_cloud", 0.7),
    // WeTransfer
    ("wetransfer.com", "WeTransfer", "personal_cloud", 0.65),
    // File.io
    ("file.io", "File.io (ephemeral)", "ephemeral", 0.8),
    // Transfer.sh
    ("transfer.sh", "Transfer.sh (ephemeral)", "ephemeral", 0.8),
    // Pastebin-like
    ("pastebin.com", "Pastebin", "paste", 0.6),
    ("paste.ee", "Paste.ee", "paste", 0.65),
    ("ghostbin.com", "Ghostbin", "paste", 0.7),
    ("privatebin.net", "PrivateBin", "paste", 0.7),
];

const SAAS_EXFIL_CHANNELS: &[(&str, &str, f64)] = &[
    ("hooks.slack.com", "Slack webhook exfiltration", 0.6),
    ("discord.com/api/webhooks", "Discord webhook exfiltration", 0.7),
    ("api.telegram.org", "Telegram bot API exfiltration", 0.7),
    ("upload.slack-edge.com", "Slack file upload", 0.5),
    ("cdn.discordapp.com", "Discord CDN upload", 0.6),
    ("files.slack.com", "Slack file storage", 0.5),
    ("notion.so", "Notion data export", 0.4),
    ("airtable.com", "Airtable data export", 0.4),
];

const SENSITIVE_FILE_PATTERNS: &[(&str, &str, f64)] = &[
    (".sql", "SQL database dump", 0.8),
    (".db", "Database file", 0.75),
    (".sqlite", "SQLite database", 0.75),
    (".csv", "CSV data export", 0.5),
    (".xlsx", "Excel spreadsheet", 0.4),
    (".pst", "Outlook mail archive", 0.8),
    (".ost", "Outlook offline archive", 0.8),
    (".mbox", "Mail archive", 0.75),
    (".key", "Cryptographic key file", 0.9),
    (".pem", "PEM certificate/key", 0.85),
    (".pfx", "PKCS12 certificate", 0.85),
    (".kdbx", "KeePass database", 0.9),
    (".1password", "1Password vault", 0.9),
    (".env", "Environment variables (secrets)", 0.85),
    (".git", "Git repository", 0.7),
    (".tar.gz", "Compressed archive", 0.5),
    (".zip", "ZIP archive", 0.4),
    (".7z", "7-Zip archive", 0.5),
    (".rar", "RAR archive", 0.5),
    (".dmg", "macOS disk image", 0.6),
    ("id_rsa", "SSH private key", 0.95),
    ("id_ed25519", "SSH ED25519 key", 0.95),
    ("credentials", "Credentials file", 0.85),
    ("password", "Password file", 0.85),
    ("secret", "Secrets file", 0.8),
    ("token", "Token file", 0.75),
    (".keychain", "macOS Keychain", 0.9),
    ("shadow", "Password shadow file", 0.95),
    ("sam", "Windows SAM database", 0.95),
    ("ntds.dit", "Active Directory database", 0.95),
];

// ── Types ────────────────────────────────────────────────────────────────────

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, serde::Serialize, serde::Deserialize)]
pub enum CloudExfilThreatType {
    LargeUpload,
    UnauthorizedCloudAccount,
    VolumeAnomaly,
    AfterHoursUpload,
    SensitiveFileUpload,
    APIKeyAbuse,
    SaaSChannelExfil,
    EncryptedArchiveUpload,
    ChunkedUpload,
    StagingDetected,
    EphemeralServiceUpload,
    PersonalCloudUpload,
    RapidSequentialUpload,
    WebhookExfil,
    PasteServiceExfil,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct CloudExfilFinding {
    pub threat_type: CloudExfilThreatType,
    pub source_ip: String,
    pub dest_domain: String,
    pub cloud_service: String,
    pub upload_size_bytes: u64,
    pub file_name: Option<String>,
    pub confidence: f64,
    pub description: String,
    pub mitre_id: String,
    pub process_name: Option<String>,
    pub process_pid: Option<u32>,
    pub username: Option<String>,
    pub timestamp: u64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct CloudUploadEvent {
    pub source_ip: String,
    pub dest_domain: String,
    pub dest_url: String,
    pub http_method: String,
    pub content_length: u64,
    pub content_type: Option<String>,
    pub file_name: Option<String>,
    pub user_agent: Option<String>,
    pub auth_header: Option<String>,
    pub process_name: Option<String>,
    pub process_pid: Option<u32>,
    pub username: Option<String>,
    pub timestamp: u64,
    pub is_encrypted: bool,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct CloudExfilResult {
    pub is_exfiltration: bool,
    pub events_analyzed: u32,
    pub findings: Vec<CloudExfilFinding>,
    pub exfil_destinations: Vec<String>,
    pub total_exfil_bytes: u64,
    pub risk_score: f64,
    pub severity: Severity,
    pub mitre_ids: Vec<String>,
    pub analysis_time_ms: u64,
}

#[derive(Debug, Clone, Default, serde::Serialize, serde::Deserialize)]
pub struct CloudExfilStats {
    pub total_events: u64,
    pub exfil_detections: u64,
    pub large_upload_detections: u64,
    pub sensitive_file_detections: u64,
    pub saas_exfil_detections: u64,
    pub total_exfil_bytes: u64,
    pub avg_analysis_time_ms: f64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct CloudExfilConfig {
    pub enabled: bool,
    pub large_upload_threshold: u64,
    pub daily_upload_threshold: u64,
    pub rapid_upload_count: u32,
    pub rapid_upload_window_secs: u64,
    pub sanctioned_domains: Vec<String>,
    pub detect_sensitive_files: bool,
    pub detect_saas_channels: bool,
    pub detect_after_hours: bool,
    pub business_hours_start: u8,
    pub business_hours_end: u8,
    pub min_confidence: f64,
    pub memory_budget_bytes: usize,
}

impl Default for CloudExfilConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            large_upload_threshold: LARGE_UPLOAD_THRESHOLD,
            daily_upload_threshold: DAILY_UPLOAD_THRESHOLD,
            rapid_upload_count: RAPID_UPLOAD_COUNT,
            rapid_upload_window_secs: RAPID_UPLOAD_WINDOW_SECS,
            sanctioned_domains: Vec::new(),
            detect_sensitive_files: true,
            detect_saas_channels: true,
            detect_after_hours: true,
            business_hours_start: 8,
            business_hours_end: 18,
            min_confidence: 0.5,
            memory_budget_bytes: 32 * 1024 * 1024,
        }
    }
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct CloudSigEntry {
    pub domain: String,
    pub service: String,
    pub severity: f64,
}

#[derive(Debug, Clone, Default)]
struct UserUploadTracker {
    uploads: Vec<(String, u64, u64)>, // (domain, size, timestamp)
    daily_bytes: u64,
    day_start: u64,
}

// ── Main Struct ──────────────────────────────────────────────────────────────

pub struct CloudExfilDetector {
    running: Arc<AtomicBool>,
    scan_history: RwLock<HierarchicalState<CloudExfilStats>>,
    result_cache: TieredCache<String, CloudExfilResult>,
    risk_computer: RwLock<ReversibleComputation<f64, f64>>,
    upload_feature_codec: RwLock<VqCodec>,
    rate_accumulator: RwLock<StreamAccumulator<f64, CloudExfilStats>>,
    metrics: MemoryMetrics,
    upload_diffs: RwLock<DifferentialStore<String, String>>,
    recent_analyses: RwLock<PruningMap<String, CloudExfilResult>>,
    cloud_sig_db: PagedMemory<CloudSigEntry>,
    file_streamer: StreamingFileProcessor,
    content_dedup: RwLock<DedupStore<String, Vec<u8>>>,
    threat_matrix: RwLock<SparseMatrix<String, String, u64>>,
    config: RwLock<CloudExfilConfig>,
    stats: RwLock<CloudExfilStats>,
    alerts: RwLock<VecDeque<MalwareAlert>>,
    total_events: AtomicU64,
    user_tracker: RwLock<HashMap<String, UserUploadTracker>>,
    compressed_artifacts: RwLock<HashMap<String, Vec<u8>>>,
}

impl CloudExfilDetector {
    pub fn new() -> Self {
        let cfg = CloudExfilConfig::default();
        Self {
            running: Arc::new(AtomicBool::new(true)),
            scan_history: RwLock::new(HierarchicalState::new(HISTORY_LEVELS, HISTORY_PER_LEVEL)),
            result_cache: TieredCache::new(CACHE_MAX),
            risk_computer: RwLock::new(ReversibleComputation::new(STATS_WINDOW, |v: &[f64]| {
                if v.is_empty() { 0.0 } else { v.iter().sum::<f64>() / v.len() as f64 }
            })),
            upload_feature_codec: RwLock::new(VqCodec::new(VQ_CODEBOOK_SIZE, VQ_VECTOR_DIM)),
            rate_accumulator: RwLock::new(StreamAccumulator::new(
                STATS_WINDOW, CloudExfilStats::default(),
                |acc: &mut CloudExfilStats, vals: &[f64]| { acc.total_events += vals.len() as u64; },
            )),
            metrics: MemoryMetrics::new(cfg.memory_budget_bytes),
            upload_diffs: RwLock::new(DifferentialStore::new()),
            recent_analyses: RwLock::new(PruningMap::new(CACHE_MAX)),
            cloud_sig_db: PagedMemory::new(PAGE_SIZE, MAX_RESIDENT_PAGES),
            file_streamer: StreamingFileProcessor::new(MMAP_CHUNK_SIZE),
            content_dedup: RwLock::new(DedupStore::new()),
            threat_matrix: RwLock::new(SparseMatrix::new(0u64)),
            config: RwLock::new(cfg),
            stats: RwLock::new(CloudExfilStats::default()),
            alerts: RwLock::new(VecDeque::with_capacity(256)),
            total_events: AtomicU64::new(0),
            user_tracker: RwLock::new(HashMap::new()),
            compressed_artifacts: RwLock::new(HashMap::new()),
        }
    }

    pub fn analyze_upload(&self, event: &CloudUploadEvent) -> Option<CloudExfilResult> {
        if !self.running.load(Ordering::SeqCst) { return None; }
        let cfg = self.config.read().clone();
        if !cfg.enabled { return None; }
        let start = std::time::Instant::now();
        self.total_events.fetch_add(1, Ordering::Relaxed);
        self.scan_history.write().checkpoint(self.stats.read().clone());

        let cache_key = format!("{}:{}:{}", event.source_ip, event.dest_domain, event.timestamp);

        let mut findings = Vec::new();
        let mut mitre_ids = HashSet::new();
        let mut total_bytes = event.content_length;

        // Identify cloud service
        let (service_name, service_type, base_conf) = self.identify_cloud_service(&event.dest_domain);

        // Check if sanctioned
        let is_sanctioned = cfg.sanctioned_domains.iter().any(|d| event.dest_domain.contains(d));
        let unsanctioned_boost = if is_sanctioned { 0.0 } else { 0.2 };

        // Large upload check
        if event.content_length > cfg.large_upload_threshold {
            findings.push(CloudExfilFinding {
                threat_type: CloudExfilThreatType::LargeUpload,
                source_ip: event.source_ip.clone(),
                dest_domain: event.dest_domain.clone(),
                cloud_service: service_name.clone(),
                upload_size_bytes: event.content_length,
                file_name: event.file_name.clone(),
                confidence: (base_conf + unsanctioned_boost + (event.content_length as f64 / (1024.0 * 1024.0 * 500.0)).min(0.2)).min(0.95),
                description: format!("Large upload: {:.1}MB to {}", event.content_length as f64 / (1024.0 * 1024.0), service_name),
                mitre_id: "T1567".into(),
                process_name: event.process_name.clone(),
                process_pid: event.process_pid,
                username: event.username.clone(),
                timestamp: event.timestamp,
            });
            mitre_ids.insert("T1567".into());
        }

        // Unauthorized cloud account
        if !is_sanctioned && !service_name.is_empty() {
            let tt = match service_type.as_str() {
                "personal_cloud" => CloudExfilThreatType::PersonalCloudUpload,
                "ephemeral" => CloudExfilThreatType::EphemeralServiceUpload,
                "paste" => CloudExfilThreatType::PasteServiceExfil,
                _ => CloudExfilThreatType::UnauthorizedCloudAccount,
            };
            let conf = match service_type.as_str() {
                "ephemeral" => 0.8,
                "paste" => 0.7,
                "personal_cloud" => 0.6,
                _ => base_conf + unsanctioned_boost,
            };
            findings.push(CloudExfilFinding {
                threat_type: tt,
                source_ip: event.source_ip.clone(),
                dest_domain: event.dest_domain.clone(),
                cloud_service: service_name.clone(),
                upload_size_bytes: event.content_length,
                file_name: event.file_name.clone(),
                confidence: conf,
                description: format!("Upload to unsanctioned {} ({})", service_name, service_type),
                mitre_id: "T1567".into(),
                process_name: event.process_name.clone(),
                process_pid: event.process_pid,
                username: event.username.clone(),
                timestamp: event.timestamp,
            });
            mitre_ids.insert("T1567".into());
        }

        // SaaS channel abuse
        if cfg.detect_saas_channels {
            for &(domain, desc, conf) in SAAS_EXFIL_CHANNELS {
                if event.dest_domain.contains(domain) || event.dest_url.contains(domain) {
                    let tt = if domain.contains("webhook") { CloudExfilThreatType::WebhookExfil }
                        else { CloudExfilThreatType::SaaSChannelExfil };
                    findings.push(CloudExfilFinding {
                        threat_type: tt,
                        source_ip: event.source_ip.clone(),
                        dest_domain: event.dest_domain.clone(),
                        cloud_service: desc.to_string(),
                        upload_size_bytes: event.content_length,
                        file_name: event.file_name.clone(),
                        confidence: conf,
                        description: desc.to_string(),
                        mitre_id: "T1567.002".into(),
                        process_name: event.process_name.clone(),
                        process_pid: event.process_pid,
                        username: event.username.clone(),
                        timestamp: event.timestamp,
                    });
                    mitre_ids.insert("T1567.002".into());
                    break;
                }
            }
        }

        // Sensitive file detection
        if cfg.detect_sensitive_files {
            if let Some(ref fname) = event.file_name {
                let fname_lower = fname.to_lowercase();
                for &(pattern, desc, conf) in SENSITIVE_FILE_PATTERNS {
                    if fname_lower.contains(&pattern.to_lowercase()) {
                        findings.push(CloudExfilFinding {
                            threat_type: CloudExfilThreatType::SensitiveFileUpload,
                            source_ip: event.source_ip.clone(),
                            dest_domain: event.dest_domain.clone(),
                            cloud_service: service_name.clone(),
                            upload_size_bytes: event.content_length,
                            file_name: Some(fname.clone()),
                            confidence: conf,
                            description: format!("{}: {} uploaded to {}", desc, fname, service_name),
                            mitre_id: "T1567".into(),
                            process_name: event.process_name.clone(),
                            process_pid: event.process_pid,
                            username: event.username.clone(),
                            timestamp: event.timestamp,
                        });
                        mitre_ids.insert("T1567".into());
                        break;
                    }
                }
            }
        }

        // Encrypted archive detection
        if event.is_encrypted {
            findings.push(CloudExfilFinding {
                threat_type: CloudExfilThreatType::EncryptedArchiveUpload,
                source_ip: event.source_ip.clone(),
                dest_domain: event.dest_domain.clone(),
                cloud_service: service_name.clone(),
                upload_size_bytes: event.content_length,
                file_name: event.file_name.clone(),
                confidence: 0.7,
                description: format!("Encrypted archive uploaded to {}", service_name),
                mitre_id: "T1560.001".into(),
                process_name: event.process_name.clone(),
                process_pid: event.process_pid,
                username: event.username.clone(),
                timestamp: event.timestamp,
            });
            mitre_ids.insert("T1560.001".into());
        }

        // Track user uploads for volume/rate anomalies
        let user_key = event.username.as_deref().unwrap_or(&event.source_ip).to_string();
        {
            let mut tracker = self.user_tracker.write();
            let ut = tracker.entry(user_key.clone()).or_default();
            ut.uploads.push((event.dest_domain.clone(), event.content_length, event.timestamp));
            ut.daily_bytes += event.content_length;
            if ut.uploads.len() > 1000 { ut.uploads.drain(..500); }

            // Daily volume check
            if ut.daily_bytes > cfg.daily_upload_threshold {
                findings.push(CloudExfilFinding {
                    threat_type: CloudExfilThreatType::VolumeAnomaly,
                    source_ip: event.source_ip.clone(),
                    dest_domain: event.dest_domain.clone(),
                    cloud_service: service_name.clone(),
                    upload_size_bytes: ut.daily_bytes,
                    file_name: None,
                    confidence: (0.6 + (ut.daily_bytes as f64 / (cfg.daily_upload_threshold as f64 * 3.0)).min(0.3)),
                    description: format!("Daily upload volume: {:.1}MB (threshold: {:.0}MB)",
                        ut.daily_bytes as f64 / (1024.0 * 1024.0),
                        cfg.daily_upload_threshold as f64 / (1024.0 * 1024.0)),
                    mitre_id: "T1567".into(),
                    process_name: event.process_name.clone(),
                    process_pid: event.process_pid,
                    username: event.username.clone(),
                    timestamp: event.timestamp,
                });
                mitre_ids.insert("T1567".into());
            }

            // Rapid sequential upload check
            let recent_uploads: Vec<_> = ut.uploads.iter()
                .filter(|&&(_, _, ts)| event.timestamp - ts < cfg.rapid_upload_window_secs)
                .collect();
            if recent_uploads.len() as u32 > cfg.rapid_upload_count {
                findings.push(CloudExfilFinding {
                    threat_type: CloudExfilThreatType::RapidSequentialUpload,
                    source_ip: event.source_ip.clone(),
                    dest_domain: event.dest_domain.clone(),
                    cloud_service: service_name.clone(),
                    upload_size_bytes: recent_uploads.iter().map(|(_, s, _)| s).sum(),
                    file_name: None,
                    confidence: 0.75,
                    description: format!("{} uploads in {}s window",
                        recent_uploads.len(), cfg.rapid_upload_window_secs),
                    mitre_id: "T1567".into(),
                    process_name: event.process_name.clone(),
                    process_pid: event.process_pid,
                    username: event.username.clone(),
                    timestamp: event.timestamp,
                });
                mitre_ids.insert("T1567".into());
            }
        }

        self.upload_diffs.write().record_insert(
            cache_key.clone(),
            format!("dest={},size={},svc={}", event.dest_domain, event.content_length, service_name),
        );

        let risk_score = self.calculate_risk_score(&findings);
        self.risk_computer.write().push(risk_score);
        let severity = Self::risk_to_severity(risk_score);
        let is_exfiltration = risk_score > 0.55;
        let mitre_vec: Vec<String> = mitre_ids.into_iter().collect();
        let elapsed = start.elapsed().as_millis() as u64;

        let exfil_dests = if is_exfiltration { vec![event.dest_domain.clone()] } else { Vec::new() };

        let result = CloudExfilResult {
            is_exfiltration,
            events_analyzed: 1,
            findings,
            exfil_destinations: exfil_dests,
            total_exfil_bytes: total_bytes,
            risk_score,
            severity: severity.clone(),
            mitre_ids: mitre_vec.clone(),
            analysis_time_ms: elapsed,
        };

        self.result_cache.insert(cache_key.clone(), result.clone());
        self.recent_analyses.write().insert_with_priority(cache_key.clone(), result.clone(), risk_score);
        {
            let mut m = self.threat_matrix.write();
            for f in &result.findings {
                let k = format!("{:?}", f.threat_type);
                let c = *m.get(&k, &cache_key);
                m.set(k, cache_key.clone(), c + 1);
            }
        }
        if let Ok(j) = serde_json::to_vec(&result.findings) {
            self.compressed_artifacts.write().insert(cache_key, compression::compress_lz4(&j));
        }
        self.rate_accumulator.write().push(risk_score);

        {
            let mut stats = self.stats.write();
            stats.total_events += 1;
            if is_exfiltration {
                stats.exfil_detections += 1;
                stats.total_exfil_bytes += total_bytes;
            }
            let n = stats.total_events as f64;
            stats.avg_analysis_time_ms = stats.avg_analysis_time_ms * ((n - 1.0) / n) + elapsed as f64 / n;
        }

        if is_exfiltration {
            self.alerts.write().push_back(MalwareAlert {
                id: uuid::Uuid::new_v4().to_string(),
                timestamp: chrono::Utc::now().timestamp(),
                severity,
                module: "cloud_exfil_detector".into(),
                title: format!("Cloud exfil: {} → {} ({})",
                    event.source_ip, event.dest_domain, service_name),
                details: format!("Risk: {:.1}%, {} findings, {:.1}MB uploaded",
                    risk_score * 100.0, result.findings.len(),
                    total_bytes as f64 / (1024.0 * 1024.0)),
                path: None,
                process_name: event.process_name.clone(),
                process_pid: event.process_pid,
                verdict: None,
                mitre_ids: mitre_vec,
                remediation: vec![
                    "Block unsanctioned cloud storage services".into(),
                    "Implement CASB for cloud usage visibility".into(),
                    "Enable DLP for outbound uploads".into(),
                    "Restrict uploads to approved cloud accounts".into(),
                ],
                confidence: risk_score,
            });
        }

        Some(result)
    }

    fn identify_cloud_service(&self, domain: &str) -> (String, String, f64) {
        let domain_lower = domain.to_lowercase();
        for &(pattern, name, svc_type, conf) in CLOUD_STORAGE_DOMAINS {
            if domain_lower.contains(&pattern.to_lowercase().replace("*", "")) {
                return (name.to_string(), svc_type.to_string(), conf);
            }
        }
        ("Unknown".into(), "unknown".into(), 0.3)
    }

    fn calculate_risk_score(&self, findings: &[CloudExfilFinding]) -> f64 {
        if findings.is_empty() { return 0.0; }
        let mut score = 0.0f64;
        for f in findings {
            let w = match f.threat_type {
                CloudExfilThreatType::SensitiveFileUpload => 1.5,
                CloudExfilThreatType::EphemeralServiceUpload => 1.4,
                CloudExfilThreatType::WebhookExfil => 1.3,
                CloudExfilThreatType::EncryptedArchiveUpload => 1.3,
                CloudExfilThreatType::VolumeAnomaly => 1.2,
                CloudExfilThreatType::RapidSequentialUpload => 1.2,
                CloudExfilThreatType::PersonalCloudUpload => 1.1,
                CloudExfilThreatType::LargeUpload => 1.0,
                _ => 0.8,
            };
            score += f.confidence * w;
        }
        (score / (findings.len() as f64 * 1.5)).min(1.0)
    }

    fn risk_to_severity(s: f64) -> Severity {
        if s >= 0.85 { Severity::Critical } else if s >= 0.65 { Severity::High }
        else if s >= 0.45 { Severity::Medium } else if s >= 0.25 { Severity::Low }
        else { Severity::Info }
    }

    pub fn stats(&self) -> CloudExfilStats { self.stats.read().clone() }
    pub fn drain_alerts(&self) -> Vec<MalwareAlert> { self.alerts.write().drain(..).collect() }
    pub fn stop(&self) { self.running.store(false, Ordering::SeqCst); }
}
