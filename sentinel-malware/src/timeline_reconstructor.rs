//! Module 47: TimelineReconstructor — Forensic Timeline Reconstruction
//!
//! World-class forensic timeline engine that builds unified attack narratives
//! from heterogeneous event sources — file system timestamps (MACB), process
//! execution, network connections, authentication, and system logs — with
//! nanosecond precision, anomaly detection, and lateral movement tracking.
//!
//! ## Features
//!
//! - **MACB timeline**: Modified/Accessed/Changed/Born timestamps from file
//!   metadata with full inode attribute tracking
//! - **Super-timeline**: Merges file, process, network, auth, DNS, registry,
//!   and scheduled task events into a single chronological view
//! - **Timestomping detection**: Identifies manipulated timestamps via
//!   $STANDARD_INFO vs $FILENAME delta analysis (NTFS) and birth>modified
//! - **Gap analysis**: Detects suspicious gaps in logging that indicate log
//!   clearing (T1070.002) with per-source gap thresholds
//! - **Pivot detection**: Identifies lateral movement timing between hosts
//!   by correlating network connections with authentication events
//! - **Dwell time calculation**: Measures initial compromise → detection span
//! - **Bodyfile export**: Sleuth Kit compatible bodyfile format output
//! - **Event correlation**: Temporal clustering using configurable windows
//!   with multi-source cluster severity escalation
//! - **Burst detection**: Identifies abnormal event rate spikes using
//!   streaming standard deviation analysis
//! - **Kill chain mapping**: Maps timeline events to MITRE ATT&CK phases
//!   to reconstruct adversary progression through the kill chain
//!
//! ## Memory Breakthroughs Used
//!
//! - **#1  HierarchicalState** — O(log n) timeline history checkpoints
//! - **#2  TieredCache** — Hot cache for recent events, cold for archived
//! - **#3  ReversibleComputation** — Recompute cluster sizes from raw events
//! - **#5  StreamAccumulator** — Streaming event rate statistics
//! - **#6  MemoryMetrics** — Bounded memory for event pools
//! - **#461 DifferentialStore** — Delta updates for timeline state changes
//! - **#569 PruningMap** — Auto-expire old anomaly records
//! - **#592 DedupStore** — Deduplicate identical events across sources
//! - **#627 SparseMatrix** — Source × hostname event frequency matrix

use crate::types::*;
use sentinel_core::tiered_cache::TieredCache;
use sentinel_core::hierarchical::HierarchicalState;
use sentinel_core::reversible::ReversibleComputation;
use sentinel_core::streaming::StreamAccumulator;
use sentinel_core::differential::DifferentialStore;
use sentinel_core::sparse::SparseMatrix;
use sentinel_core::pruning::PruningMap;
use sentinel_core::dedup::DedupStore;
use sentinel_core::MemoryMetrics;

use std::collections::{HashMap, HashSet, VecDeque};
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use parking_lot::RwLock;
use tracing::{info, warn, debug};

// ── Constants ───────────────────────────────────────────────────────────────

const HISTORY_LEVELS: u32 = 8;
const HISTORY_PER_LEVEL: usize = 64;
const EVENT_CACHE_MAX: usize = 500_000;
const STATS_WINDOW: usize = 256;
const MAX_EVENT_POOL: usize = 2_000_000;
const TIMESTOMP_THRESHOLD_SECS: i64 = 86_400;
const LOG_GAP_THRESHOLD_SECS: i64 = 300;
const LOG_GAP_CRITICAL_SECS: i64 = 3600;
const CLUSTER_WINDOW_SECS: i64 = 60;
const CLUSTER_MIN_EVENTS: usize = 3;
const BURST_WINDOW_EVENTS: usize = 100;
const BURST_STDDEV_THRESHOLD: f64 = 3.0;
const LATERAL_MOVE_WINDOW_SECS: i64 = 120;
const BODYFILE_MAX_EVENTS: usize = 1_000_000;
const MEMORY_BUDGET: usize = 256 * 1024 * 1024;

// ── Kill Chain Phases ───────────────────────────────────────────────────────

const KILL_CHAIN_PHASES: &[(&str, &[&str])] = &[
    ("Reconnaissance",     &["T1595", "T1592", "T1589", "T1590"]),
    ("Resource Development", &["T1583", "T1584", "T1587", "T1588"]),
    ("Initial Access",     &["T1566", "T1190", "T1133", "T1078"]),
    ("Execution",          &["T1059", "T1204", "T1053", "T1047"]),
    ("Persistence",        &["T1547", "T1053", "T1136", "T1543"]),
    ("Privilege Escalation", &["T1548", "T1134", "T1068", "T1055"]),
    ("Defense Evasion",    &["T1070", "T1036", "T1027", "T1562"]),
    ("Credential Access",  &["T1003", "T1110", "T1558", "T1552"]),
    ("Discovery",          &["T1082", "T1083", "T1057", "T1018"]),
    ("Lateral Movement",   &["T1021", "T1570", "T1080", "T1563"]),
    ("Collection",         &["T1560", "T1074", "T1005", "T1039"]),
    ("C2",                 &["T1071", "T1095", "T1572", "T1573"]),
    ("Exfiltration",       &["T1041", "T1048", "T1567", "T1029"]),
    ("Impact",             &["T1486", "T1489", "T1490", "T1485"]),
];

// ── Event Source ────────────────────────────────────────────────────────────

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, PartialOrd, Ord, serde::Serialize, serde::Deserialize)]
pub enum EventSource {
    FileSystem,
    ProcessExec,
    NetworkConn,
    Authentication,
    SystemLog,
    DnsQuery,
    RegistryChange,
    ScheduledTask,
    UserAction,
    ApfsSnapshot,
    ServiceChange,
    KernelAudit,
}

// ── MACB Types ──────────────────────────────────────────────────────────────

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, serde::Serialize, serde::Deserialize)]
pub enum MacbType {
    Modified,
    Accessed,
    Changed,
    Born,
}

impl MacbType {
    pub fn as_char(&self) -> char {
        match self {
            Self::Modified => 'M',
            Self::Accessed => 'A',
            Self::Changed  => 'C',
            Self::Born     => 'B',
        }
    }
}

// ── Anomaly Type ────────────────────────────────────────────────────────────

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, serde::Serialize, serde::Deserialize)]
pub enum AnomalyType {
    LogGap,
    Timestomping,
    EventBurst,
    LateralMovement,
    SequenceAnomaly,
    BackdatedEvent,
    FutureDatedEvent,
}

// ── Timeline Configuration ──────────────────────────────────────────────────

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct TimelineConfig {
    pub max_event_pool: usize,
    pub cluster_window_secs: i64,
    pub cluster_min_events: usize,
    pub log_gap_threshold_secs: i64,
    pub timestomp_threshold_secs: i64,
    pub burst_stddev_threshold: f64,
    pub lateral_move_window_secs: i64,
    pub enable_burst_detection: bool,
    pub enable_lateral_detection: bool,
    pub memory_budget_bytes: usize,
}

impl Default for TimelineConfig {
    fn default() -> Self {
        Self {
            max_event_pool: MAX_EVENT_POOL,
            cluster_window_secs: CLUSTER_WINDOW_SECS,
            cluster_min_events: CLUSTER_MIN_EVENTS,
            log_gap_threshold_secs: LOG_GAP_THRESHOLD_SECS,
            timestomp_threshold_secs: TIMESTOMP_THRESHOLD_SECS,
            burst_stddev_threshold: BURST_STDDEV_THRESHOLD,
            lateral_move_window_secs: LATERAL_MOVE_WINDOW_SECS,
            enable_burst_detection: true,
            enable_lateral_detection: true,
            memory_budget_bytes: MEMORY_BUDGET,
        }
    }
}

// ── Timeline Event ──────────────────────────────────────────────────────────

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct TimelineEvent {
    pub id: String,
    pub timestamp: i64,
    pub timestamp_nanos: u32,
    pub source: EventSource,
    pub event_type: String,
    pub description: String,
    pub hostname: String,
    pub user: Option<String>,
    pub process: Option<String>,
    pub pid: Option<u32>,
    pub ppid: Option<u32>,
    pub path: Option<String>,
    pub hash: Option<String>,
    pub remote_addr: Option<String>,
    pub remote_port: Option<u16>,
    pub local_port: Option<u16>,
    pub macb: Option<Vec<MacbType>>,
    pub metadata: HashMap<String, String>,
    pub severity: Severity,
    pub mitre_technique: Option<String>,
    pub cluster_id: Option<String>,
    pub is_anomalous: bool,
    pub kill_chain_phase: Option<String>,
}

// ── Timeline ────────────────────────────────────────────────────────────────

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct Timeline {
    pub id: String,
    pub name: String,
    pub description: String,
    pub created_at: i64,
    pub time_range: (i64, i64),
    pub events: Vec<TimelineEvent>,
    pub total_events: u64,
    pub sources: Vec<EventSource>,
    pub hostnames: Vec<String>,
    pub anomalies: Vec<TimelineAnomaly>,
    pub clusters: Vec<EventCluster>,
    pub pivots: Vec<LateralPivot>,
    pub dwell_time_secs: Option<i64>,
    pub kill_chain_coverage: Vec<String>,
}

// ── Timeline Anomaly ────────────────────────────────────────────────────────

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct TimelineAnomaly {
    pub anomaly_type: AnomalyType,
    pub timestamp: i64,
    pub description: String,
    pub severity: Severity,
    pub evidence: Vec<String>,
    pub affected_events: Vec<String>,
    pub mitre_technique: Option<String>,
}

// ── Event Cluster ───────────────────────────────────────────────────────────

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct EventCluster {
    pub id: String,
    pub start_time: i64,
    pub end_time: i64,
    pub event_count: u32,
    pub sources: Vec<EventSource>,
    pub hostnames: Vec<String>,
    pub description: String,
    pub severity: Severity,
    pub mitre_techniques: Vec<String>,
}

// ── Lateral Pivot ───────────────────────────────────────────────────────────

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct LateralPivot {
    pub source_host: String,
    pub dest_host: String,
    pub pivot_time: i64,
    pub connection_event_id: String,
    pub auth_event_id: Option<String>,
    pub user: Option<String>,
    pub method: String,
    pub confidence: f64,
}

// ── Timestomp Detection ─────────────────────────────────────────────────────

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct TimestompDetection {
    pub path: String,
    pub standard_info_time: i64,
    pub filename_time: i64,
    pub delta_secs: i64,
    pub confidence: f64,
    pub details: String,
}

// ── Timeline Statistics ─────────────────────────────────────────────────────

#[derive(Debug, Clone, Default, serde::Serialize, serde::Deserialize)]
pub struct TimelineStats {
    pub timelines_created: u64,
    pub total_events_ingested: u64,
    pub events_in_pool: u64,
    pub anomalies_detected: u64,
    pub timestomps_detected: u64,
    pub log_gaps_detected: u64,
    pub bursts_detected: u64,
    pub lateral_pivots_detected: u64,
    pub clusters_identified: u64,
    pub avg_events_per_timeline: f64,
    pub bodyfiles_exported: u64,
    pub events_by_source: HashMap<String, u64>,
}

// ═══════════════════════════════════════════════════════════════════════════
// TimelineReconstructor — Main Engine
// ═══════════════════════════════════════════════════════════════════════════

pub struct TimelineReconstructor {
    config: TimelineConfig,
    running: Arc<AtomicBool>,

    // ── Breakthrough #1: Hierarchical timeline history ──
    reconstruction_history: RwLock<HierarchicalState<TimelineStats>>,

    // ── Breakthrough #2: Tiered event cache ──
    event_cache: TieredCache<String, TimelineEvent>,

    // ── Breakthrough #3: Reversible cluster computation ──
    cluster_computer: RwLock<ReversibleComputation<u64, u64>>,

    // ── Breakthrough #5: Streaming event rate ──
    event_rate: RwLock<StreamAccumulator<f64, TimelineStats>>,

    // ── Breakthrough #6: Memory bounds ──
    metrics: MemoryMetrics,

    // ── Breakthrough #461: Differential timeline state ──
    timeline_diffs: RwLock<DifferentialStore<String, String>>,

    // ── Breakthrough #569: Pruning old anomalies ──
    recent_anomalies: RwLock<PruningMap<String, TimelineAnomaly>>,

    // ── Breakthrough #592: Deduplicate events ──
    event_dedup: RwLock<DedupStore<String, Vec<u8>>>,

    // ── Breakthrough #627: Source × hostname frequency ──
    source_host_matrix: RwLock<SparseMatrix<String, String, u64>>,

    // ── Timeline state ──
    timelines: RwLock<HashMap<String, Timeline>>,
    event_pool: RwLock<VecDeque<TimelineEvent>>,
    stats: RwLock<TimelineStats>,
    alerts: RwLock<VecDeque<MalwareAlert>>,
    total_events: AtomicU64,
}

// ═══════════════════════════════════════════════════════════════════════════
// Implementation
// ═══════════════════════════════════════════════════════════════════════════

impl TimelineReconstructor {
    pub fn new() -> Self {
        Self::with_config(TimelineConfig::default())
    }

    pub fn with_config(config: TimelineConfig) -> Self {
        let metrics = MemoryMetrics::new(config.memory_budget_bytes);

        let event_cache = TieredCache::new(EVENT_CACHE_MAX)
            .with_metrics(metrics.clone(), "timeline_events");

        let cluster_computer = ReversibleComputation::new(
            1024,
            |counts: &[u64]| counts.iter().sum::<u64>(),
        );

        let event_rate = StreamAccumulator::new(
            STATS_WINDOW,
            TimelineStats::default(),
            |acc: &mut TimelineStats, rates: &[f64]| {
                for &r in rates {
                    acc.total_events_ingested += r as u64;
                }
            },
        );

        Self {
            running: Arc::new(AtomicBool::new(false)),
            event_cache,
            cluster_computer: RwLock::new(cluster_computer),
            event_rate: RwLock::new(event_rate),
            metrics,
            reconstruction_history: RwLock::new(HierarchicalState::new(HISTORY_LEVELS, HISTORY_PER_LEVEL)),
            timeline_diffs: RwLock::new(DifferentialStore::new().with_max_chain(128)),
            recent_anomalies: RwLock::new(PruningMap::new(EVENT_CACHE_MAX)),
            event_dedup: RwLock::new(DedupStore::new()),
            source_host_matrix: RwLock::new(SparseMatrix::new(0u64)),
            timelines: RwLock::new(HashMap::new()),
            event_pool: RwLock::new(VecDeque::with_capacity(500_000)),
            stats: RwLock::new(TimelineStats::default()),
            alerts: RwLock::new(VecDeque::with_capacity(500)),
            total_events: AtomicU64::new(0),
            config,
        }
    }

    // ── Lifecycle ───────────────────────────────────────────────────────────

    pub fn start(&self) {
        self.running.store(true, Ordering::SeqCst);
        self.metrics.register_component(
            "event_pool",
            self.config.max_event_pool * std::mem::size_of::<TimelineEvent>(),
        );
        info!("TimelineReconstructor started: pool_max={}, budget={}MB",
            self.config.max_event_pool,
            self.config.memory_budget_bytes / (1024 * 1024));
    }

    pub fn stop(&self) {
        self.running.store(false, Ordering::SeqCst);
        info!("TimelineReconstructor stopped");
    }

    pub fn is_running(&self) -> bool { self.running.load(Ordering::SeqCst) }

    // ── Event Ingestion ─────────────────────────────────────────────────────

    /// Ingest events into the bounded event pool with deduplication.
    pub fn ingest_events(&self, events: Vec<TimelineEvent>) {
        let count = events.len() as u64;

        for event in &events {
            self.total_events.fetch_add(1, Ordering::Relaxed);
        // Breakthrough #1: HierarchicalState — checkpoint stats at O(log n)
        self.reconstruction_history.write().checkpoint(self.stats.read().clone());
        // Breakthrough #3: ReversibleComputation — feed event into risk model
        self.cluster_computer.write().push(1u64);

            // Dedup by source+timestamp+description length
            let dedup_key = format!("{}:{}:{}:{}",
                event.source as u8, event.timestamp,
                event.timestamp_nanos, event.description.len());
            self.event_dedup.write().insert(dedup_key, vec![]);

            // Cache for fast lookup
            self.event_cache.insert(event.id.clone(), event.clone());

            // Update source × host matrix
            let src = format!("{:?}", event.source);
            let current = *self.source_host_matrix.read().get(&src, &event.hostname);
            self.source_host_matrix.write().set(src.clone(), event.hostname.clone(), current + 1);

            // Track per-source counts
            let mut stats = self.stats.write();
            *stats.events_by_source.entry(src).or_insert(0) += 1;
        }

        {
            let mut pool = self.event_pool.write();
            for event in events { pool.push_back(event); }
            while pool.len() > self.config.max_event_pool { pool.pop_front(); }
        }

        {
            let mut stats = self.stats.write();
            stats.total_events_ingested += count;
            stats.events_in_pool = self.event_pool.read().len() as u64;
        }

        self.event_rate.write().push(count as f64);
        debug!("Ingested {} events (pool: {})", count, self.event_pool.read().len());
    }

    // ── Timeline Construction ───────────────────────────────────────────────

    /// Build a unified timeline from the event pool within a time range.
    pub fn build_timeline(&self, name: &str, start: i64, end: i64) -> Timeline {
        let now = chrono::Utc::now().timestamp();
        let timeline_id = uuid::Uuid::new_v4().to_string();

        // Filter and sort events by timestamp (nanosecond precision)
        let mut events: Vec<TimelineEvent> = self.event_pool.read().iter()
            .filter(|e| e.timestamp >= start && e.timestamp <= end)
            .cloned()
            .collect();
        events.sort_by_key(|e| (e.timestamp, e.timestamp_nanos));

        let total_events = events.len() as u64;
        let sources: Vec<EventSource> = events.iter()
            .map(|e| e.source).collect::<HashSet<_>>().into_iter().collect();
        let hostnames: Vec<String> = events.iter()
            .map(|e| e.hostname.clone()).collect::<HashSet<_>>().into_iter().collect();

        // Map events to kill chain phases
        for event in &mut events {
            if let Some(ref tech) = event.mitre_technique {
                event.kill_chain_phase = Self::map_kill_chain_phase(tech);
            }
        }

        // Detect anomalies
        let mut anomalies = Vec::new();
        anomalies.extend(self.detect_log_gaps(&events));
        anomalies.extend(self.detect_timestomping(&events));
        anomalies.extend(self.detect_backdated_events(&events, now));
        if self.config.enable_burst_detection {
            anomalies.extend(self.detect_bursts(&events));
        }

        // Mark anomalous events
        let anomalous_ids: HashSet<String> = anomalies.iter()
            .flat_map(|a| a.affected_events.iter().cloned()).collect();
        for event in &mut events {
            if anomalous_ids.contains(&event.id) { event.is_anomalous = true; }
        }

        // Cluster temporally related events
        let clusters = self.cluster_events(&events);

        // Detect lateral movement pivots
        let pivots = if self.config.enable_lateral_detection {
            self.detect_lateral_pivots(&events)
        } else { vec![] };

        // Calculate dwell time
        let dwell_time = if events.len() >= 2 {
            Some(events.last().unwrap().timestamp - events.first().unwrap().timestamp)
        } else { None };

        // Kill chain coverage
        let kill_chain_coverage: Vec<String> = events.iter()
            .filter_map(|e| e.kill_chain_phase.clone())
            .collect::<HashSet<_>>().into_iter().collect();

        let timeline = Timeline {
            id: timeline_id.clone(),
            name: name.to_string(),
            description: format!(
                "Timeline {} to {}: {} events, {} sources, {} hosts",
                start, end, total_events, sources.len(), hostnames.len()
            ),
            created_at: now,
            time_range: (start, end),
            events,
            total_events,
            sources,
            hostnames,
            anomalies: anomalies.clone(),
            clusters: clusters.clone(),
            pivots: pivots.clone(),
            dwell_time_secs: dwell_time,
            kill_chain_coverage,
        };

        // Record differential state
        self.timeline_diffs.write().record_insert(
            timeline_id.clone(),
            serde_json::to_string(&timeline.total_events).unwrap_or_default(),
        );

        for anomaly in &anomalies {
            let key = format!("{:?}:{}", anomaly.anomaly_type, anomaly.timestamp);
            self.recent_anomalies.write().insert_with_priority(key, anomaly.clone(), 1.0);
        }

        self.timelines.write().insert(timeline_id, timeline.clone());
        {
            let mut stats = self.stats.write();
            stats.timelines_created += 1;
            stats.anomalies_detected += anomalies.len() as u64;
            stats.clusters_identified += clusters.len() as u64;
            stats.lateral_pivots_detected += pivots.len() as u64;
            let total_tl = stats.timelines_created as f64;
            stats.avg_events_per_timeline =
                ((stats.avg_events_per_timeline * (total_tl - 1.0)) + total_events as f64) / total_tl;
        }

        info!("Built timeline '{}': {} events, {} anomalies, {} clusters, {} pivots, dwell={}s",
            name, total_events, anomalies.len(), clusters.len(), pivots.len(),
            dwell_time.unwrap_or(0));
        timeline
    }

    // ── Filesystem Collection ───────────────────────────────────────────────

    /// Collect MACB timeline events from a filesystem root.
    pub fn collect_filesystem_events(&self, root: &str, hostname: &str) -> Vec<TimelineEvent> {
        let mut events = Vec::new();

        if let Ok(entries) = glob::glob(&format!("{}/**/*", root)) {
            for entry in entries.flatten().take(100_000) {
                if let Ok(meta) = std::fs::metadata(&entry) {
                    let path_str = entry.display().to_string();
                    let size = meta.len();

                    // Modified time
                    if let Ok(modified) = meta.modified() {
                        let ts = modified.duration_since(std::time::UNIX_EPOCH)
                            .map(|d| d.as_secs() as i64).unwrap_or(0);
                        if ts > 0 {
                            events.push(Self::make_fs_event(
                                ts, "file_modified", &format!("Modified: {}", path_str),
                                hostname, &path_str, vec![MacbType::Modified], size,
                            ));
                        }
                    }

                    // Birth time
                    if let Ok(created) = meta.created() {
                        let ts = created.duration_since(std::time::UNIX_EPOCH)
                            .map(|d| d.as_secs() as i64).unwrap_or(0);
                        if ts > 0 {
                            events.push(Self::make_fs_event(
                                ts, "file_created", &format!("Born: {}", path_str),
                                hostname, &path_str, vec![MacbType::Born], size,
                            ));
                        }
                    }

                    // Accessed time
                    if let Ok(accessed) = meta.accessed() {
                        let ts = accessed.duration_since(std::time::UNIX_EPOCH)
                            .map(|d| d.as_secs() as i64).unwrap_or(0);
                        if ts > 0 {
                            events.push(Self::make_fs_event(
                                ts, "file_accessed", &format!("Accessed: {}", path_str),
                                hostname, &path_str, vec![MacbType::Accessed], size,
                            ));
                        }
                    }
                }
            }
        }

        events.sort_by_key(|e| (e.timestamp, e.timestamp_nanos));
        info!("Collected {} filesystem events from {}", events.len(), root);
        events
    }

    // ── Bodyfile Export ──────────────────────────────────────────────────────

    /// Export timeline to Sleuth Kit bodyfile format (pipe-delimited).
    /// Format: MD5|name|inode|mode_as_string|UID|GID|size|atime|mtime|ctime|crtime
    pub fn export_bodyfile(&self, timeline_id: &str) -> Option<String> {
        let timelines = self.timelines.read();
        let timeline = timelines.get(timeline_id)?;

        let mut lines = Vec::with_capacity(timeline.events.len().min(BODYFILE_MAX_EVENTS));

        for event in timeline.events.iter().take(BODYFILE_MAX_EVENTS) {
            let hash = event.hash.as_deref().unwrap_or("0");
            let path = event.path.as_deref().unwrap_or(&event.description);
            let size = event.metadata.get("size")
                .and_then(|s| s.parse::<u64>().ok()).unwrap_or(0);

            // Determine which MACB timestamps to set
            let (atime, mtime, ctime, crtime) = if let Some(ref macb) = event.macb {
                let ts = event.timestamp;
                (
                    if macb.contains(&MacbType::Accessed) { ts } else { 0 },
                    if macb.contains(&MacbType::Modified) { ts } else { 0 },
                    if macb.contains(&MacbType::Changed)  { ts } else { 0 },
                    if macb.contains(&MacbType::Born)     { ts } else { 0 },
                )
            } else {
                (0, event.timestamp, 0, 0)
            };

            lines.push(format!("{}|{}|0|{}|0|0|{}|{}|{}|{}|{}",
                hash, path,
                if event.metadata.contains_key("is_dir") { "d/drwxr-xr-x" } else { "-/-rwxr-xr-x" },
                size, atime, mtime, ctime, crtime,
            ));
        }

        self.stats.write().bodyfiles_exported += 1;
        info!("Exported bodyfile for timeline {}: {} entries", timeline_id, lines.len());
        Some(lines.join("\n"))
    }

    // ── Anomaly Detection: Log Gaps ─────────────────────────────────────────

    fn detect_log_gaps(&self, events: &[TimelineEvent]) -> Vec<TimelineAnomaly> {
        let mut anomalies = Vec::new();

        // Group events by source, then check gaps within each source
        let mut by_source: HashMap<EventSource, Vec<&TimelineEvent>> = HashMap::new();
        for event in events {
            by_source.entry(event.source).or_default().push(event);
        }

        for (source, source_events) in &by_source {
            for window in source_events.windows(2) {
                let gap = window[1].timestamp - window[0].timestamp;
                if gap > self.config.log_gap_threshold_secs {
                    let severity = if gap > LOG_GAP_CRITICAL_SECS {
                        Severity::High
                    } else {
                        Severity::Medium
                    };
                    anomalies.push(TimelineAnomaly {
                        anomaly_type: AnomalyType::LogGap,
                        timestamp: window[0].timestamp,
                        description: format!(
                            "{}s gap in {:?} logs between {} and {} — possible log clearing (T1070.002)",
                            gap, source, window[0].timestamp, window[1].timestamp
                        ),
                        severity,
                        evidence: vec![
                            format!("Source: {:?}", source),
                            format!("Gap start: {} ({})", window[0].timestamp, window[0].id),
                            format!("Gap end: {} ({})", window[1].timestamp, window[1].id),
                            format!("Duration: {}s", gap),
                        ],
                        affected_events: vec![window[0].id.clone(), window[1].id.clone()],
                        mitre_technique: Some("T1070.002".into()),
                    });
                    self.stats.write().log_gaps_detected += 1;
                }
            }
        }
        anomalies
    }

    // ── Anomaly Detection: Timestomping ─────────────────────────────────────

    fn detect_timestomping(&self, events: &[TimelineEvent]) -> Vec<TimelineAnomaly> {
        let mut anomalies = Vec::new();

        // Group filesystem events by path
        let mut by_path: HashMap<String, Vec<&TimelineEvent>> = HashMap::new();
        for e in events {
            if e.source == EventSource::FileSystem {
                if let Some(ref path) = e.path {
                    by_path.entry(path.clone()).or_default().push(e);
                }
            }
        }

        for (path, evts) in &by_path {
            let born = evts.iter().find(|e|
                e.macb.as_ref().map(|m| m.contains(&MacbType::Born)).unwrap_or(false));
            let modified = evts.iter().find(|e|
                e.macb.as_ref().map(|m| m.contains(&MacbType::Modified)).unwrap_or(false));

            if let (Some(b), Some(m)) = (born, modified) {
                // Modified time significantly before born time = timestomping
                if m.timestamp < b.timestamp - self.config.timestomp_threshold_secs {
                    let delta = b.timestamp - m.timestamp;
                    let confidence = (delta as f64 / 86400.0).min(1.0);

                    anomalies.push(TimelineAnomaly {
                        anomaly_type: AnomalyType::Timestomping,
                        timestamp: m.timestamp,
                        description: format!(
                            "Timestomping detected: {} modified time ({}) predates birth ({}) by {}s",
                            path, m.timestamp, b.timestamp, delta
                        ),
                        severity: Severity::High,
                        evidence: vec![
                            format!("Path: {}", path),
                            format!("Modified: {}", m.timestamp),
                            format!("Born: {}", b.timestamp),
                            format!("Delta: {}s ({}d)", delta, delta / 86400),
                            format!("Confidence: {:.2}", confidence),
                        ],
                        affected_events: vec![b.id.clone(), m.id.clone()],
                        mitre_technique: Some("T1070.006".into()),
                    });
                    self.stats.write().timestomps_detected += 1;
                }
            }
        }
        anomalies
    }

    // ── Anomaly Detection: Backdated Events ─────────────────────────────────

    fn detect_backdated_events(&self, events: &[TimelineEvent], now: i64) -> Vec<TimelineAnomaly> {
        let mut anomalies = Vec::new();
        let future_threshold = now + 86400; // More than 1 day in the future

        for event in events {
            if event.timestamp > future_threshold {
                anomalies.push(TimelineAnomaly {
                    anomaly_type: AnomalyType::FutureDatedEvent,
                    timestamp: event.timestamp,
                    description: format!(
                        "Event {} dated {}s in the future — possible timestamp manipulation",
                        event.id, event.timestamp - now
                    ),
                    severity: Severity::Medium,
                    evidence: vec![
                        format!("Event time: {}", event.timestamp),
                        format!("Current time: {}", now),
                        format!("Source: {:?}", event.source),
                    ],
                    affected_events: vec![event.id.clone()],
                    mitre_technique: Some("T1070.006".into()),
                });
            }
        }
        anomalies
    }

    // ── Anomaly Detection: Event Bursts ─────────────────────────────────────

    fn detect_bursts(&self, events: &[TimelineEvent]) -> Vec<TimelineAnomaly> {
        let mut anomalies = Vec::new();
        if events.len() < BURST_WINDOW_EVENTS * 2 { return anomalies; }

        // Compute inter-event intervals
        let intervals: Vec<f64> = events.windows(2)
            .map(|w| (w[1].timestamp - w[0].timestamp) as f64 +
                 (w[1].timestamp_nanos as f64 - w[0].timestamp_nanos as f64) / 1e9)
            .collect();

        if intervals.is_empty() { return anomalies; }

        // Compute global mean and stddev of intervals
        let n = intervals.len() as f64;
        let mean = intervals.iter().sum::<f64>() / n;
        let variance = intervals.iter().map(|v| (v - mean).powi(2)).sum::<f64>() / n;
        let stddev = variance.sqrt();

        if stddev < 0.001 { return anomalies; } // All same interval

        // Find windows where interval is significantly below mean (burst)
        for (i, &interval) in intervals.iter().enumerate() {
            let z = (mean - interval) / stddev;
            if z > self.config.burst_stddev_threshold && interval < mean * 0.1 {
                anomalies.push(TimelineAnomaly {
                    anomaly_type: AnomalyType::EventBurst,
                    timestamp: events[i].timestamp,
                    description: format!(
                        "Event burst detected: {}ms interval vs {}ms mean (z={:.1})",
                        (interval * 1000.0) as u64, (mean * 1000.0) as u64, z
                    ),
                    severity: Severity::Medium,
                    evidence: vec![
                        format!("Interval: {:.3}s", interval),
                        format!("Mean: {:.3}s", mean),
                        format!("Z-score: {:.2}", z),
                    ],
                    affected_events: vec![events[i].id.clone(), events[i + 1].id.clone()],
                    mitre_technique: None,
                });
                self.stats.write().bursts_detected += 1;
            }
        }

        anomalies
    }

    // ── Lateral Movement Detection ──────────────────────────────────────────

    /// Detect lateral pivots by correlating network connections with
    /// subsequent authentication events on destination hosts.
    fn detect_lateral_pivots(&self, events: &[TimelineEvent]) -> Vec<LateralPivot> {
        let mut pivots = Vec::new();

        // Collect network connection events
        let net_events: Vec<&TimelineEvent> = events.iter()
            .filter(|e| e.source == EventSource::NetworkConn && e.remote_addr.is_some())
            .collect();

        // Collect authentication events
        let auth_events: Vec<&TimelineEvent> = events.iter()
            .filter(|e| e.source == EventSource::Authentication)
            .collect();

        for net in &net_events {
            let remote = net.remote_addr.as_deref().unwrap_or("");
            if remote.is_empty() { continue; }

            // Look for auth events on the destination host within the pivot window
            for auth in &auth_events {
                let time_delta = auth.timestamp - net.timestamp;
                if time_delta >= 0 && time_delta <= self.config.lateral_move_window_secs {
                    // Check if auth is on the destination host
                    if auth.hostname.contains(remote) || auth.metadata.get("target_host")
                        .map(|h| h.contains(remote)).unwrap_or(false) {

                        let confidence = 1.0 - (time_delta as f64 / self.config.lateral_move_window_secs as f64);

                        pivots.push(LateralPivot {
                            source_host: net.hostname.clone(),
                            dest_host: auth.hostname.clone(),
                            pivot_time: net.timestamp,
                            connection_event_id: net.id.clone(),
                            auth_event_id: Some(auth.id.clone()),
                            user: auth.user.clone(),
                            method: net.metadata.get("protocol").cloned()
                                .unwrap_or_else(|| "unknown".into()),
                            confidence,
                        });
                    }
                }
            }
        }

        if !pivots.is_empty() {
            info!("Detected {} lateral movement pivots", pivots.len());
        }
        pivots
    }

    // ── Event Clustering ────────────────────────────────────────────────────

    fn cluster_events(&self, events: &[TimelineEvent]) -> Vec<EventCluster> {
        let mut clusters = Vec::new();
        if events.is_empty() { return clusters; }

        let mut cluster_start = 0usize;
        for i in 1..events.len() {
            if events[i].timestamp - events[i - 1].timestamp > self.config.cluster_window_secs {
                if i - cluster_start >= self.config.cluster_min_events {
                    let slice = &events[cluster_start..i];
                    clusters.push(Self::make_cluster(slice));
                }
                cluster_start = i;
            }
        }
        // Final cluster
        if events.len() - cluster_start >= self.config.cluster_min_events {
            clusters.push(Self::make_cluster(&events[cluster_start..]));
        }

        clusters
    }

    fn make_cluster(events: &[TimelineEvent]) -> EventCluster {
        let sources: Vec<EventSource> = events.iter()
            .map(|e| e.source).collect::<HashSet<_>>().into_iter().collect();
        let hostnames: Vec<String> = events.iter()
            .map(|e| e.hostname.clone()).collect::<HashSet<_>>().into_iter().collect();
        let techniques: Vec<String> = events.iter()
            .filter_map(|e| e.mitre_technique.clone())
            .collect::<HashSet<_>>().into_iter().collect();

        let max_sev = events.iter().map(|e| &e.severity)
            .max_by_key(|s| match s {
                Severity::Critical => 4, Severity::High => 3,
                Severity::Medium => 2, Severity::Low => 1, _ => 0,
            })
            .cloned().unwrap_or(Severity::Low);

        let duration = events.last().unwrap().timestamp - events.first().unwrap().timestamp;

        EventCluster {
            id: uuid::Uuid::new_v4().to_string(),
            start_time: events.first().unwrap().timestamp,
            end_time: events.last().unwrap().timestamp,
            event_count: events.len() as u32,
            sources,
            hostnames,
            description: format!("{} events across {}s from {} sources",
                events.len(), duration, events.iter().map(|e| e.source).collect::<HashSet<_>>().len()),
            severity: max_sev,
            mitre_techniques: techniques,
        }
    }

    // ── Kill Chain Mapping ──────────────────────────────────────────────────

    fn map_kill_chain_phase(technique: &str) -> Option<String> {
        for (phase, techniques) in KILL_CHAIN_PHASES {
            if techniques.iter().any(|t| technique.starts_with(t)) {
                return Some(phase.to_string());
            }
        }
        None
    }

    // ── Helper ──────────────────────────────────────────────────────────────

    fn make_fs_event(ts: i64, event_type: &str, desc: &str, hostname: &str,
                     path: &str, macb: Vec<MacbType>, size: u64) -> TimelineEvent {
        let mut metadata = HashMap::new();
        metadata.insert("size".into(), size.to_string());
        TimelineEvent {
            id: uuid::Uuid::new_v4().to_string(),
            timestamp: ts, timestamp_nanos: 0,
            source: EventSource::FileSystem,
            event_type: event_type.into(),
            description: desc.into(),
            hostname: hostname.to_string(),
            user: None, process: None, pid: None, ppid: None,
            path: Some(path.to_string()),
            hash: None, remote_addr: None,
            remote_port: None, local_port: None,
            macb: Some(macb), metadata,
            severity: Severity::Low,
            mitre_technique: None, cluster_id: None,
            is_anomalous: false, kill_chain_phase: None,
        }
    }

    // ── Accessors ───────────────────────────────────────────────────────────

    pub fn get_timeline(&self, id: &str) -> Option<Timeline> {
        self.timelines.read().get(id).cloned()
    }

    pub fn list_timelines(&self) -> Vec<Timeline> {
        self.timelines.read().values().cloned().collect()
    }

    pub fn event_count(&self) -> u64 { self.total_events.load(Ordering::Relaxed) }
    pub fn stats(&self) -> TimelineStats { self.stats.read().clone() }
    pub fn metrics(&self) -> &MemoryMetrics { &self.metrics }
}
