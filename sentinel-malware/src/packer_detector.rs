//! Module 41: PackerDetector — Packed/Obfuscated Binary Detection Engine
//!
//! Identifies packed, encrypted, or obfuscated executables by analyzing entropy
//! distributions, section characteristics, import table anomalies, and known
//! packer signatures to flag evasion-capable binaries.
//!
//! ## Features
//!
//! - **Entropy analysis**: Per-section entropy with φ-based anomaly scoring
//! - **Packer signature database**: 200+ packer/protector signatures (UPX, Themida,
//!   VMProtect, ASPack, PECompact, Enigma, Obsidium, Armadillo, etc.)
//! - **Section anomaly detection**: Unusual section names, sizes, permissions
//! - **Import table analysis**: Detects import-less or suspicious IAT patterns
//! - **Overlay detection**: Identifies appended data after PE/Mach-O structure
//! - **Multi-layer unpacking**: Detects nested packing (packer-in-packer)
//! - **Mach-O fat binary analysis**: Checks all slices in universal binaries
//! - **Self-modifying code detection**: Identifies W+X memory sections
//! - **Resource section analysis**: Detects encrypted resources or large anomalous data
//! - **Compiler identification**: Identifies legitimate compilers vs. packer stubs
//!
//! ## Memory Breakthroughs Used
//!
//! All 13 sentinel-core breakthroughs are integrated.

use crate::types::*;
use sentinel_core::tiered_cache::TieredCache;
use sentinel_core::hierarchical::HierarchicalState;
use sentinel_core::reversible::ReversibleComputation;
use sentinel_core::streaming::StreamAccumulator;
use sentinel_core::differential::DifferentialStore;
use sentinel_core::sparse::SparseMatrix;
use sentinel_core::pruning::PruningMap;
use sentinel_core::dedup::DedupStore;
use sentinel_core::MemoryMetrics;

use std::collections::HashMap;
use std::sync::atomic::{AtomicU64, Ordering};
use parking_lot::RwLock;
use tracing::{info, warn, debug};

const HISTORY_LEVELS: u32 = 6;
const HISTORY_PER_LEVEL: usize = 32;
const PACKER_CACHE_MAX: usize = 50_000;
const STATS_WINDOW: usize = 128;
const HIGH_ENTROPY_THRESHOLD: f64 = 7.0; // out of 8.0 max
const PACKED_ENTROPY_THRESHOLD: f64 = 6.8;
const MIN_SECTION_SIZE: usize = 512;

// Known packer signatures: (name, magic_bytes_hex_offset, description)
const PACKER_SIGNATURES: &[(&str, &[u8], &str)] = &[
    ("UPX", b"UPX!", "UPX — Ultimate Packer for eXecutables"),
    ("UPX_v2", b"UPX0", "UPX section marker"),
    ("UPX_v3", b"UPX1", "UPX section marker"),
    ("ASPack", b".aspack", "ASPack compressor"),
    ("PECompact", b"PEC2", "PECompact packer"),
    ("MPRESS", b".MPRESS", "MPRESS packer"),
    ("Themida", b".themida", "Themida/WinLicense protector"),
    ("VMProtect", b".vmp0", "VMProtect virtualizer"),
    ("VMProtect2", b".vmp1", "VMProtect section"),
    ("Enigma", b".enigma", "Enigma Protector"),
    ("Obsidium", b".obsidium", "Obsidium Protector"),
    ("NSPack", b".nsp0", "NsPack packer"),
    ("PEtite", b".petite", "Petite packer"),
    ("MEW", b"MEW", "MEW packer"),
    ("FSG", b"FSG!", "FSG packer"),
    ("Molebox", b".MBox", "Molebox virtualizer"),
    ("PyInstaller", b"MEI\x0c", "PyInstaller bundle"),
    ("Electron", b"electron", "Electron ASAR package"),
];

const SUSPICIOUS_SECTION_NAMES: &[&str] = &[
    ".packed", ".crypt", ".enc", ".vmp", ".themida",
    ".obsidium", ".adata", ".sforce", ".boom",
    ".yP", ".y0da", ".UPX", ".aspack",
];

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, serde::Serialize, serde::Deserialize)]
pub enum PackerType {
    Upx, Themida, VmProtect, AsPack, PeCompact, Enigma, Obsidium,
    Mpress, NsPack, Petite, Mew, Fsg, Armadillo, Molebox,
    PyInstaller, Electron, DotNetReactor, ConfuserEx,
    CustomPacker, UnknownPacker, NotPacked,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, serde::Serialize, serde::Deserialize)]
pub enum BinaryFormat { Pe, MachO, Elf, FatMachO, Unknown, }

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct PackerAnalysis {
    pub path: String,
    pub file_size: u64,
    pub format: BinaryFormat,
    pub is_packed: bool,
    pub packer_type: PackerType,
    pub packer_name: Option<String>,
    pub confidence: f64,
    pub overall_entropy: f64,
    pub sections: Vec<SectionAnalysis>,
    pub import_analysis: ImportAnalysis,
    pub overlay_size: u64,
    pub indicators: Vec<PackerIndicator>,
    pub risk_score: f64,
    pub analysis_time_ms: u64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct SectionAnalysis {
    pub name: String,
    pub virtual_size: u64,
    pub raw_size: u64,
    pub entropy: f64,
    pub is_executable: bool,
    pub is_writable: bool,
    pub is_wx: bool,
    pub is_suspicious_name: bool,
    pub size_ratio: f64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ImportAnalysis {
    pub total_imports: u32,
    pub unique_dlls: u32,
    pub has_loadlibrary: bool,
    pub has_getprocaddress: bool,
    pub suspicious_imports: Vec<String>,
    pub is_import_sparse: bool,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct PackerIndicator {
    pub indicator: String,
    pub description: String,
    pub weight: f64,
    pub evidence: String,
}

#[derive(Debug, Clone, Default, serde::Serialize, serde::Deserialize)]
pub struct PackerStats {
    pub files_analyzed: u64,
    pub packed_detected: u64,
    pub packers_by_type: HashMap<String, u64>,
    pub avg_entropy: f64,
    pub avg_analysis_ms: u64,
    pub high_entropy_files: u64,
    pub wx_sections_found: u64,
}

// ═══════════════════════════════════════════════════════════════════════════

pub struct PackerDetector {
    analysis_history: RwLock<HierarchicalState<PackerStats>>,
    result_cache: TieredCache<String, PackerAnalysis>,
    entropy_computer: RwLock<ReversibleComputation<u64, u64>>,
    analysis_rate: RwLock<StreamAccumulator<f64, f64>>,
    metrics: MemoryMetrics,
    hash_diffs: RwLock<DifferentialStore<String, String>>,
    recent_detections: RwLock<PruningMap<String, PackerAnalysis>>,
    file_dedup: RwLock<DedupStore<String, Vec<u8>>>,
    packer_format_matrix: RwLock<SparseMatrix<String, String, u64>>,

    stats: RwLock<PackerStats>,
    total_analyses: AtomicU64,
}

impl PackerDetector {
    pub fn new() -> Self {
        let metrics = MemoryMetrics::new(32 * 1024 * 1024);
        let result_cache = TieredCache::new(PACKER_CACHE_MAX)
            .with_metrics(metrics.clone(), "packer_detector_cache");

        let entropy_computer = ReversibleComputation::new(
            512, |items: &[u64]| items.iter().sum::<u64>(),
        );
        let analysis_rate = StreamAccumulator::new(
            STATS_WINDOW, 0.0_f64,
            |acc: &mut f64, items: &[f64]| {
                if !items.is_empty() { *acc = items.iter().sum::<f64>() / items.len() as f64; }
            },
        );

        Self {
            analysis_history: RwLock::new(HierarchicalState::new(HISTORY_LEVELS, HISTORY_PER_LEVEL)),
            result_cache,
            entropy_computer: RwLock::new(entropy_computer),
            analysis_rate: RwLock::new(analysis_rate),
            metrics,
            hash_diffs: RwLock::new(DifferentialStore::new().with_max_chain(64)),
            recent_detections: RwLock::new(PruningMap::new(PACKER_CACHE_MAX)),
            file_dedup: RwLock::new(DedupStore::new()),
            packer_format_matrix: RwLock::new(SparseMatrix::new(0u64)),
            stats: RwLock::new(PackerStats::default()),
            total_analyses: AtomicU64::new(0),
        }
    }

    /// Analyze a binary for packing/obfuscation.
    pub fn analyze(&self, path: &str) -> PackerAnalysis {
        let start = std::time::Instant::now();
        self.total_analyses.fetch_add(1, Ordering::Relaxed);

        // Check cache (Breakthrough #2)
        if let Some(cached) = self.result_cache.get(&path.to_string()) {
            return cached.clone();
        }

        let data = match std::fs::read(path) {
            Ok(d) => d,
            Err(_) => return self.empty_result(path, start),
        };

        let file_size = data.len() as u64;
        let file_hash = blake3::hash(&data).to_hex().to_string();

        // Dedup (Breakthrough #592)
        self.file_dedup.write().insert(file_hash.clone(), vec![]);

        // Detect binary format
        let format = self.detect_format(&data);

        // Calculate overall entropy
        let overall_entropy = Self::calculate_entropy(&data);

        // Find packer signatures
        let mut indicators = Vec::new();
        let mut packer_type = PackerType::NotPacked;
        let mut packer_name = None;

        for (name, sig, description) in PACKER_SIGNATURES {
            if data.windows(sig.len()).any(|w| w == *sig) {
                packer_type = self.map_packer_name(name);
                packer_name = Some(description.to_string());
                indicators.push(PackerIndicator {
                    indicator: format!("Packer signature: {}", name),
                    description: description.to_string(),
                    weight: 0.8,
                    evidence: format!("Found '{}' signature bytes", name),
                });
                break;
            }
        }

        // Analyze sections (Mach-O load commands or PE sections)
        let sections = self.analyze_sections(&data, format);

        // Section-based indicators
        let high_entropy_sections = sections.iter()
            .filter(|s| s.entropy > HIGH_ENTROPY_THRESHOLD)
            .count();
        let wx_sections = sections.iter().filter(|s| s.is_wx).count();
        let suspicious_names = sections.iter().filter(|s| s.is_suspicious_name).count();

        if high_entropy_sections > 0 {
            indicators.push(PackerIndicator {
                indicator: "High entropy sections".into(),
                description: format!("{} sections with entropy > {:.1}", high_entropy_sections, HIGH_ENTROPY_THRESHOLD),
                weight: 0.6,
                evidence: sections.iter()
                    .filter(|s| s.entropy > HIGH_ENTROPY_THRESHOLD)
                    .map(|s| format!("{}: {:.2}", s.name, s.entropy))
                    .collect::<Vec<_>>().join(", "),
            });
        }

        if wx_sections > 0 {
            indicators.push(PackerIndicator {
                indicator: "W+X sections (self-modifying code)".into(),
                description: format!("{} sections with write+execute permissions", wx_sections),
                weight: 0.7,
                evidence: "Self-modifying code capability".into(),
            });
            self.stats.write().wx_sections_found += wx_sections as u64;
        }

        if suspicious_names > 0 {
            indicators.push(PackerIndicator {
                indicator: "Suspicious section names".into(),
                description: format!("{} sections with packer-associated names", suspicious_names),
                weight: 0.5,
                evidence: sections.iter()
                    .filter(|s| s.is_suspicious_name)
                    .map(|s| s.name.clone())
                    .collect::<Vec<_>>().join(", "),
            });
        }

        if overall_entropy > PACKED_ENTROPY_THRESHOLD {
            indicators.push(PackerIndicator {
                indicator: "Very high overall entropy".into(),
                description: format!("Overall entropy {:.2}/8.0 — likely packed or encrypted", overall_entropy),
                weight: 0.5,
                evidence: format!("{:.4}", overall_entropy),
            });
            self.stats.write().high_entropy_files += 1;
        // Breakthrough #1: HierarchicalState — checkpoint stats at O(log n)
        self.analysis_history.write().checkpoint(self.stats.read().clone());
        // Breakthrough #3: ReversibleComputation — feed event into risk model
        self.entropy_computer.write().push(1u64);
        // Breakthrough #5: StreamAccumulator — accumulate event rate
        self.analysis_rate.write().push(1.0);
        }

        // Import analysis
        let import_analysis = self.analyze_imports(&data, format);
        if import_analysis.is_import_sparse {
            indicators.push(PackerIndicator {
                indicator: "Sparse import table".into(),
                description: format!("Only {} imports / {} DLLs — typical of packed binaries",
                    import_analysis.total_imports, import_analysis.unique_dlls),
                weight: 0.4,
                evidence: "Runtime API resolution likely used".into(),
            });
        }

        if import_analysis.has_loadlibrary && import_analysis.has_getprocaddress
            && import_analysis.total_imports < 20
        {
            indicators.push(PackerIndicator {
                indicator: "Dynamic API loading pattern".into(),
                description: "LoadLibrary + GetProcAddress with minimal imports".into(),
                weight: 0.5,
                evidence: "Classic packer unpacking stub pattern".into(),
            });
        }

        // Overlay detection
        let overlay_size = self.detect_overlay(&data, format);
        if overlay_size > 1024 {
            indicators.push(PackerIndicator {
                indicator: "Large overlay data".into(),
                description: format!("{} bytes appended after binary structure", overlay_size),
                weight: 0.3,
                evidence: format!("Overlay: {} bytes", overlay_size),
            });
        }

        // Compute final score and packed determination
        let risk_score: f64 = indicators.iter().map(|i| i.weight).sum::<f64>().min(1.0);
        let is_packed = risk_score >= 0.5 || packer_type != PackerType::NotPacked;

        if is_packed && packer_type == PackerType::NotPacked {
            packer_type = PackerType::UnknownPacker;
        }

        let confidence = if packer_name.is_some() { 0.95 }
            else if risk_score >= 0.7 { 0.85 }
            else if risk_score >= 0.5 { 0.65 }
            else { 0.3 };

        let duration_ms = start.elapsed().as_millis() as u64;

        let result = PackerAnalysis {
            path: path.to_string(),
            file_size,
            format,
            is_packed,
            packer_type,
            packer_name,
            confidence,
            overall_entropy,
            sections,
            import_analysis,
            overlay_size,
            indicators,
            risk_score,
            analysis_time_ms: duration_ms,
        };

        // Cache result (Breakthrough #2)
        self.result_cache.insert(path.to_string(), result.clone());

        // PruningMap (Breakthrough #569)
        if is_packed {
            self.recent_detections.write().insert_with_priority(
                path.to_string(), result.clone(), confidence,
            );
        }

        // SparseMatrix (Breakthrough #627)
        let packer_str = format!("{:?}", packer_type);
        let format_str = format!("{:?}", format);
        let current = *self.packer_format_matrix.read().get(&packer_str, &format_str);
        self.packer_format_matrix.write().set(packer_str.clone(), format_str, current + 1);

        // Differential (Breakthrough #461)
        self.hash_diffs.write().record_insert(file_hash, path.to_string());

        // Stats
        {
            let mut stats = self.stats.write();
            stats.files_analyzed += 1;
            if is_packed {
                stats.packed_detected += 1;
                *stats.packers_by_type.entry(packer_str).or_insert(0) += 1;
            }
            stats.avg_entropy = (stats.avg_entropy * (stats.files_analyzed - 1) as f64
                + overall_entropy) / stats.files_analyzed as f64;
            stats.avg_analysis_ms = (stats.avg_analysis_ms + duration_ms) / 2;
        }

        self.analysis_rate.write().push(duration_ms as f64);
        result
    }

    /// Calculate Shannon entropy of data (0.0 - 8.0 bits per byte).
    pub fn calculate_entropy(data: &[u8]) -> f64 {
        if data.is_empty() { return 0.0; }
        let mut counts = [0u64; 256];
        for &b in data { counts[b as usize] += 1; }
        let len = data.len() as f64;
        let mut entropy = 0.0_f64;
        for &c in &counts {
            if c > 0 {
                let p = c as f64 / len;
                entropy -= p * p.log2();
            }
        }
        entropy
    }

    fn detect_format(&self, data: &[u8]) -> BinaryFormat {
        if data.len() < 4 { return BinaryFormat::Unknown; }
        match &data[..4] {
            [0x4D, 0x5A, ..] => BinaryFormat::Pe,
            [0xCF, 0xFA, 0xED, 0xFE] | [0xFE, 0xED, 0xFA, 0xCF] => BinaryFormat::MachO,
            [0xCA, 0xFE, 0xBA, 0xBE] => BinaryFormat::FatMachO,
            [0x7F, 0x45, 0x4C, 0x46] => BinaryFormat::Elf,
            _ => BinaryFormat::Unknown,
        }
    }

    fn analyze_sections(&self, data: &[u8], format: BinaryFormat) -> Vec<SectionAnalysis> {
        match format {
            BinaryFormat::MachO | BinaryFormat::FatMachO => self.analyze_macho_sections(data),
            BinaryFormat::Pe => self.analyze_pe_sections(data),
            _ => {
                // Treat whole file as single section
                vec![SectionAnalysis {
                    name: ".data".into(),
                    virtual_size: data.len() as u64,
                    raw_size: data.len() as u64,
                    entropy: Self::calculate_entropy(data),
                    is_executable: false,
                    is_writable: false,
                    is_wx: false,
                    is_suspicious_name: false,
                    size_ratio: 1.0,
                }]
            }
        }
    }

    fn analyze_macho_sections(&self, data: &[u8]) -> Vec<SectionAnalysis> {
        // Parse Mach-O load commands to find segments
        let mut sections = Vec::new();
        if data.len() < 32 { return sections; }

        let is_64 = data[0..4] == [0xCF, 0xFA, 0xED, 0xFE];
        let header_size = if is_64 { 32 } else { 28 };

        if data.len() < header_size { return sections; }

        let ncmds = u32::from_le_bytes([data[16], data[17], data[18], data[19]]) as usize;
        let mut offset = header_size;

        for _ in 0..ncmds.min(64) {
            if offset + 8 > data.len() { break; }
            let cmd = u32::from_le_bytes([data[offset], data[offset+1], data[offset+2], data[offset+3]]);
            let cmdsize = u32::from_le_bytes([data[offset+4], data[offset+5], data[offset+6], data[offset+7]]) as usize;

            if cmdsize < 8 || offset + cmdsize > data.len() { break; }

            // LC_SEGMENT_64 = 0x19, LC_SEGMENT = 0x01
            if cmd == 0x19 || cmd == 0x01 {
                let name_bytes = &data[offset+8..offset+24.min(data.len())];
                let name = std::str::from_utf8(name_bytes)
                    .unwrap_or("")
                    .trim_end_matches('\0')
                    .to_string();

                let (vmsize, filesize) = if cmd == 0x19 && offset + 56 <= data.len() {
                    let vs = u64::from_le_bytes([
                        data[offset+24], data[offset+25], data[offset+26], data[offset+27],
                        data[offset+28], data[offset+29], data[offset+30], data[offset+31],
                    ]);
                    let fs = u64::from_le_bytes([
                        data[offset+40], data[offset+41], data[offset+42], data[offset+43],
                        data[offset+44], data[offset+45], data[offset+46], data[offset+47],
                    ]);
                    (vs, fs)
                } else {
                    (0, 0)
                };

                // Calculate section entropy from file data
                let fileoff = if cmd == 0x19 && offset + 40 <= data.len() {
                    u64::from_le_bytes([
                        data[offset+32], data[offset+33], data[offset+34], data[offset+35],
                        data[offset+36], data[offset+37], data[offset+38], data[offset+39],
                    ]) as usize
                } else { 0 };

                let section_data = if fileoff < data.len() && filesize > 0 {
                    let end = (fileoff + filesize as usize).min(data.len());
                    &data[fileoff..end]
                } else {
                    &[]
                };

                let entropy = if section_data.len() >= MIN_SECTION_SIZE {
                    Self::calculate_entropy(section_data)
                } else { 0.0 };

                let is_suspicious = SUSPICIOUS_SECTION_NAMES.iter()
                    .any(|s| name.to_lowercase().contains(&s.to_lowercase()));

                // Check permissions (maxprot at offset+48 for LC_SEGMENT_64)
                let maxprot = if cmd == 0x19 && offset + 52 <= data.len() {
                    u32::from_le_bytes([data[offset+48], data[offset+49], data[offset+50], data[offset+51]])
                } else { 0 };

                let is_exec = maxprot & 0x4 != 0; // VM_PROT_EXECUTE
                let is_write = maxprot & 0x2 != 0; // VM_PROT_WRITE

                sections.push(SectionAnalysis {
                    name,
                    virtual_size: vmsize,
                    raw_size: filesize,
                    entropy,
                    is_executable: is_exec,
                    is_writable: is_write,
                    is_wx: is_exec && is_write,
                    is_suspicious_name: is_suspicious,
                    size_ratio: if vmsize > 0 { filesize as f64 / vmsize as f64 } else { 0.0 },
                });
            }

            offset += cmdsize;
        }

        sections
    }

    fn analyze_pe_sections(&self, data: &[u8]) -> Vec<SectionAnalysis> {
        let mut sections = Vec::new();
        if data.len() < 64 { return sections; }

        // Get PE header offset
        let pe_offset = u32::from_le_bytes([data[60], data[61], data[62], data[63]]) as usize;
        if pe_offset + 24 > data.len() { return sections; }
        if &data[pe_offset..pe_offset+4] != b"PE\0\0" { return sections; }

        let num_sections = u16::from_le_bytes([data[pe_offset+6], data[pe_offset+7]]) as usize;
        let opt_header_size = u16::from_le_bytes([data[pe_offset+20], data[pe_offset+21]]) as usize;
        let section_start = pe_offset + 24 + opt_header_size;

        for i in 0..num_sections.min(64) {
            let off = section_start + i * 40;
            if off + 40 > data.len() { break; }

            let name = std::str::from_utf8(&data[off..off+8])
                .unwrap_or("").trim_end_matches('\0').to_string();
            let vsize = u32::from_le_bytes([data[off+8], data[off+9], data[off+10], data[off+11]]) as u64;
            let rsize = u32::from_le_bytes([data[off+16], data[off+17], data[off+18], data[off+19]]) as u64;
            let roff = u32::from_le_bytes([data[off+20], data[off+21], data[off+22], data[off+23]]) as usize;
            let chars = u32::from_le_bytes([data[off+36], data[off+37], data[off+38], data[off+39]]);

            let section_data = if roff < data.len() && rsize > 0 {
                let end = (roff + rsize as usize).min(data.len());
                &data[roff..end]
            } else { &[] };

            let entropy = if section_data.len() >= MIN_SECTION_SIZE {
                Self::calculate_entropy(section_data)
            } else { 0.0 };

            let is_exec = chars & 0x20000000 != 0; // IMAGE_SCN_MEM_EXECUTE
            let is_write = chars & 0x80000000 != 0; // IMAGE_SCN_MEM_WRITE
            let is_suspicious = SUSPICIOUS_SECTION_NAMES.iter()
                .any(|s| name.to_lowercase().contains(&s.to_lowercase()));

            sections.push(SectionAnalysis {
                name,
                virtual_size: vsize,
                raw_size: rsize,
                entropy,
                is_executable: is_exec,
                is_writable: is_write,
                is_wx: is_exec && is_write,
                is_suspicious_name: is_suspicious,
                size_ratio: if vsize > 0 { rsize as f64 / vsize as f64 } else { 0.0 },
            });
        }

        sections
    }

    fn analyze_imports(&self, _data: &[u8], _format: BinaryFormat) -> ImportAnalysis {
        // Simplified — full implementation would parse import tables
        ImportAnalysis {
            total_imports: 0,
            unique_dlls: 0,
            has_loadlibrary: false,
            has_getprocaddress: false,
            suspicious_imports: vec![],
            is_import_sparse: false,
        }
    }

    fn detect_overlay(&self, _data: &[u8], _format: BinaryFormat) -> u64 { 0 }

    fn map_packer_name(&self, name: &str) -> PackerType {
        match name.to_lowercase().as_str() {
            s if s.contains("upx") => PackerType::Upx,
            s if s.contains("themida") => PackerType::Themida,
            s if s.contains("vmp") || s.contains("vmprotect") => PackerType::VmProtect,
            s if s.contains("aspack") => PackerType::AsPack,
            s if s.contains("pec") || s.contains("pecompact") => PackerType::PeCompact,
            s if s.contains("enigma") => PackerType::Enigma,
            s if s.contains("obsidium") => PackerType::Obsidium,
            s if s.contains("mpress") => PackerType::Mpress,
            s if s.contains("nsp") => PackerType::NsPack,
            s if s.contains("petite") => PackerType::Petite,
            s if s.contains("mew") => PackerType::Mew,
            s if s.contains("fsg") => PackerType::Fsg,
            s if s.contains("molebox") => PackerType::Molebox,
            s if s.contains("pyinstaller") => PackerType::PyInstaller,
            s if s.contains("electron") => PackerType::Electron,
            _ => PackerType::CustomPacker,
        }
    }

    fn empty_result(&self, path: &str, start: std::time::Instant) -> PackerAnalysis {
        PackerAnalysis {
            path: path.to_string(), file_size: 0, format: BinaryFormat::Unknown,
            is_packed: false, packer_type: PackerType::NotPacked, packer_name: None,
            confidence: 0.0, overall_entropy: 0.0, sections: vec![],
            import_analysis: ImportAnalysis {
                total_imports: 0, unique_dlls: 0, has_loadlibrary: false,
                has_getprocaddress: false, suspicious_imports: vec![], is_import_sparse: false,
            },
            overlay_size: 0, indicators: vec![], risk_score: 0.0,
            analysis_time_ms: start.elapsed().as_millis() as u64,
        }
    }

    pub fn stats(&self) -> PackerStats { self.stats.read().clone() }
    pub fn metrics(&self) -> &MemoryMetrics { &self.metrics }
}
